{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "from pathlib import Path\n",
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "duration = 2.97\n",
    "sr = 22050 # if sampling rate is different, resample it to this\n",
    "input_root = './../../../models/'\n",
    "input_whole = input_root + 'cnn_dong_model_whole.h5'\n",
    "input_label = input_root + 'label.pkl'\n",
    "default_song = Path(\"../../../audio/testfiles/GTZAN/genres/pop/pop.00001.wav\")\n",
    "fmax = 10000 # maximum frequency considered\n",
    "fft_window_points = 512\n",
    "fft_window_dur = fft_window_points * 1.0 / sr # 23ms windows\n",
    "hop_size = int(fft_window_points/ 2) # 50% overlap between consecutive frames\n",
    "n_mels = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(input_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_song(song = default_song, offset = 0):\n",
    "    offset = duration*offset\n",
    "    y, sr = librosa.load(song, mono=True, offset=offset, duration=duration)\n",
    "    m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_window_points,\n",
    "                                              hop_length=hop_size, n_mels=n_mels,\n",
    "                                              fmax=fmax)\n",
    "    m_sp = np.expand_dims(m_sp, 0)\n",
    "    m_sp = np.expand_dims(m_sp, 3)\n",
    "    return m_sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_label, 'rb') as f:\n",
    "    lb = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(1, 64, 256, 1)\n",
      "[[1.9037491e-03 1.9418549e-09 6.2142976e-07 3.1947952e-02 1.7766736e-01\n",
      "  6.6977037e-09 9.7064622e-05 7.8751349e-01 7.4354600e-04 1.2622590e-04]]\n",
      "[7]\n"
     ]
    }
   ],
   "source": [
    "song = load_song()\n",
    "#np.expand_dims(song, axis=1)\n",
    "#song.append([1])\n",
    "print(type(song))\n",
    "print(np.shape(song))\n",
    "prediction = model.predict(song)\n",
    "print(prediction)\n",
    "print(prediction.argmax(axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['pop']\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(song)\n",
    "print(lb.classes_[prediction.argmax(axis=-1)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

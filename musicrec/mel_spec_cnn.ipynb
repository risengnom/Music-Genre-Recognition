{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.use(\"Agg\")\n",
    "%matplotlib inline\n",
    "\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import regularizers\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "    \n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import argmax\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle, random, os, sys, json, re, getopt, warnings\n",
    "\n",
    "#Visualization\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "#env parameters\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "#Folders\n",
    "data_folder = Path(\"../../../audio/testfiles/GTZAN/genres/\")\n",
    "output_root = './../../../models/'\n",
    "output_folder = Path('./output/cvnn.model')\n",
    "output_model = output_root + '/cnn_dong_model_weights.h5'\n",
    "output_architecture = output_root + '/cnn_dong_model_architecture.json'\n",
    "output_whole = output_root + 'cnn_dong_model_whole.h5'\n",
    "output_best_model = output_root + 'best_model.h5'\n",
    "output_label = output_root + 'label.pkl'\n",
    "output_test_paths = output_root + 'test_paths.pkl'\n",
    "\n",
    "# Duration of songsnippet in seconds\n",
    "duration = 2.97\n",
    "start_offset = 0\n",
    "epochs = 4\n",
    "num_segments = 19\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "es_patience = 20\n",
    "return_train_and_test = 1\n",
    "\n",
    "sr = 22050 # Sampling rate\n",
    "\n",
    "#Parameters for mel spec\n",
    "fmax = 1500 # maximum frequency considered\n",
    "fft_points = 512\n",
    "fft_dur = fft_points * 1.0 / sr # 23ms windows\n",
    "hop_size = int(fft_points/ 2) # 50% overlap between consecutive frames\n",
    "n_mels = 64\n",
    "\n",
    "#Segment duration\n",
    "num_fft_windows = 256 #per Segment\n",
    "segment_in_points = num_fft_windows * 255\n",
    "segment_dur = segment_in_points * 1.0 / sr\n",
    "\n",
    "input_shape=(64, 256, 1)\n",
    "\n",
    "randomseed = 11\n",
    "#randomseed = datetime.now()\n",
    "# Seed for RNG\n",
    "random.seed(randomseed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directories of all songs\n",
    "songs = []\n",
    "genres = []\n",
    "\n",
    "for g in data_folder.iterdir():\n",
    "    genres.append(g.name)\n",
    "    for i in g.iterdir():\n",
    "        songs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Loaded and shuffled 1000 songs from 10 genres.\n"
     ]
    }
   ],
   "source": [
    "#shuffle songs to keep segements together\n",
    "random.shuffle(songs)\n",
    "print(\"[Info]: Loaded and shuffled \" + str(len(songs)) + \" songs from \" + str(len(genres)) + \" genres.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading with different numbers of segements for data-augmentation\n",
    "#!Initialize \"spectograms\" before running\n",
    "def load_specs(data = songs, num_segments = 1):\n",
    "    spectograms = []\n",
    "    for song in data:\n",
    "        offset = start_offset\n",
    "        for i in range(num_segments):\n",
    "            y, sr = librosa.load(song, mono=True, offset=offset, duration=duration)\n",
    "            m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_points,\n",
    "                                                  hop_length=hop_size, n_mels=n_mels,\n",
    "                                                  fmax=fmax)\n",
    "            label = song.parts[-2]\n",
    "            spectograms.append( (m_sp, label, song, offset, duration) )\n",
    "            offset = offset + duration/2\n",
    "        input_shape = m_sp.shape + (1,)\n",
    "    return spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info]: Generating Spectograms for 19 segments per song.\n"
     ]
    }
   ],
   "source": [
    "print(\"[Info]: Generating Spectograms for \" + str(num_segments) + \" segments per song.\")\n",
    "spectograms = load_specs(data = songs, num_segments = num_segments)\n",
    "labels = []\n",
    "for i in spectograms:\n",
    "    labels.append(i[1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Info]: Total number of samples: \", len(mel_specs))\n",
    "\n",
    "#Set dynamic input shape\n",
    "input_shape = np.shape(spectograms[0][0]) + (1,)\n",
    "print(\"Input shape: \" + str(input_shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Info]: Splitting into train and testing data.\")\n",
    "#Split into Train and Testing\n",
    "testsplit = len(spectograms)*0.7    #70% train-test-split\n",
    "train = spectograms[:int(testsplit)]\n",
    "test = spectograms[int(testsplit):]\n",
    "\n",
    "x_train, y_train, p_train, offset_train, duration_train = zip(*train)\n",
    "x_test, y_test, p_test, offset_test, duration_test = zip(*test)\n",
    "\n",
    "#Fit Dimensions\n",
    "x_train = np.array([x.reshape(input_shape) for x in x_train])\n",
    "x_test = np.array([x.reshape(input_shape) for x in x_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binarize Labels\n",
    "lb = LabelBinarizer()\n",
    "y_train = lb.fit_transform(y_train)\n",
    "y_test = lb.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_testspecs_labels():\n",
    "    with open(output_label, 'wb') as f:\n",
    "        pickle.dump(lb, f)\n",
    "\n",
    "    if return_train_and_test == 1:\n",
    "        r_paths = p_test + (p_train)\n",
    "        r_offsets = offset_test + (offset_train)\n",
    "        r_durations = duration_test + (duration_train)\n",
    "    else:\n",
    "        r_paths = p_test\n",
    "        r_offsets = offset_test\n",
    "        r_durations = duration_test\n",
    "\n",
    "    r_values = [r_paths, r_offsets, r_durations]\n",
    "\n",
    "    with open(output_test_paths, 'wb') as f:\n",
    "        pickle.dump(r_values, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Info]: Dumping data for prediction.\")\n",
    "dump_testspecs_labels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model for GTZAN\n",
    "def cnn_gtzan_model_build():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 5), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(len(genres), activation='softmax'))\n",
    "    #model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "     #             optimizer=keras.optimizers.Adadelta(decay=1e-6),\n",
    "      #            metrics=['accuracy'])\n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model\n",
    "\n",
    "def train_model():\n",
    "    model = cnn_gtzan_model_build()\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=es_patience),\n",
    "                 ModelCheckpoint(filepath=output_best_model, monitor='val_loss', save_best_only=True)]\n",
    "\n",
    "    H = model.fit(\n",
    "        x=x_train, \n",
    "        y=y_train,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        batch_size=batch_size,\n",
    "        validation_data= (x_test, y_test))\n",
    "\n",
    "    score = model.evaluate(x=x_test,y=y_test)\n",
    "\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])\n",
    "    return model\n",
    "\n",
    "def save_model():\n",
    "    # Save the weights\n",
    "    model.save_weights(output_model)\n",
    "\n",
    "    # Save the model architecture\n",
    "    with open(output_architecture, 'w') as f:\n",
    "        f.write(model.to_json())\n",
    "\n",
    "    # Save complete model\n",
    "    model.save(output_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Info]: Training model.\")\n",
    "model = train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"[Info]: Saving model.\")\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_song(song, model=model, duration=duration):\n",
    "    start = 0\n",
    "    r = 0\n",
    "    for i in range(int(30/(duration/2))-1):\n",
    "        y, sr = librosa.load(song, mono=True, offset=start, duration=duration)\n",
    "        m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_points,\n",
    "                                              hop_length=hop_size, n_mels=n_mels,\n",
    "                                              fmax=fmax)\n",
    "        m_sp = np.expand_dims(m_sp, 0)\n",
    "        m_sp = np.expand_dims(m_sp, 3)\n",
    "        if r == 0:\n",
    "            prediction = model.predict(m_sp)\n",
    "            r = 1\n",
    "        else:\n",
    "            prediction = (prediction + model.predict(m_sp))/2\n",
    "        start = start + duration/2\n",
    "    return prediction\n",
    "        \n",
    "def evaluate_batch(batch):\n",
    "    predictions = []\n",
    "    for song in batch:\n",
    "        predictions.append(evaluate_song(song))\n",
    "    return predictions\n",
    "\n",
    "def compare_batch(predictions):\n",
    "    i = 0\n",
    "    l = lb.inverse_transform(y_test)\n",
    "    counter = 0\n",
    "    for prediction in predictions:\n",
    "        tr_value = str(l[i])\n",
    "        pr_value = re.sub('[\\[\\]\\']', '', str(lb.classes_[prediction.argmax(axis=1)]))\n",
    "        print(tr_value + \"=>\" + pr_value)\n",
    "        if tr_value == pr_value:\n",
    "            counter += 1\n",
    "        i += 1\n",
    "    acc = counter/len(predictions)*100\n",
    "    print(\"Acc: \" + str(acc) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(output_whole)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = evaluate_batch(p_test)\n",
    "compare_batch(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(x_test, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), \n",
    "                            target_names=lb.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(lb.inverse_transform(y_test), lb.classes_[predictions.argmax(axis=1)], genres)\n",
    "\n",
    "cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm = np.around(cm, decimals=2)\n",
    "print(genres)\n",
    "print(cm)\n",
    "print(lb.classes_[predictions.argmax(axis=1)])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_array = [[0.34,0.03,0.23,0.08,0.02,0.03,0.02,0.09,0.16,0.01],\n",
    " [0.02, 0.68, 0.02, 0.1 , 0.  , 0.01, 0.04, 0.02, 0.02, 0.08],\n",
    " [0.09, 0.04, 0.63, 0.03, 0.07, 0.02, 0.01, 0.1 , 0.01, 0.  ],\n",
    " [0.  , 0.03, 0.06, 0.67, 0.01, 0.02, 0.05, 0.12, 0.  , 0.04],\n",
    " [0.02, 0.  , 0.06, 0.02, 0.61, 0.1 , 0.01, 0.15, 0.02, 0.  ],\n",
    " [0.  , 0.  , 0.01, 0.  , 0.02, 0.97, 0.  , 0.  , 0.  , 0.  ],\n",
    " [0.02, 0.04, 0.03, 0.03, 0.03, 0.04, 0.58, 0.07, 0.02, 0.15],\n",
    " [0.09, 0.  , 0.06, 0.02, 0.07, 0.02, 0.02, 0.63, 0.09, 0.  ],\n",
    " [0.17, 0.03, 0.01, 0.01, 0.  , 0.  , 0.01, 0.04, 0.71, 0.03],\n",
    " [0.01, 0.12, 0.  , 0.08, 0.  , 0.  , 0.12, 0.02, 0.12, 0.54]]\n",
    "\n",
    "df_cm = pd.DataFrame(p_array, genres,\n",
    "                  genres)\n",
    "plt.figure(figsize = (10,7))\n",
    "sn.set(font_scale=1.4)#for label size\n",
    "\n",
    "ax=sn.heatmap(df_cm, annot=True,annot_kws={\"size\": 16}, cmap=\"BuGn\")# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

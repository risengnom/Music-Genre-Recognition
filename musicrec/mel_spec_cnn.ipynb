{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#TODO: cleanup, lint\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import librosa\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "    \n",
    "# import the necessary packages\n",
    "from musicrec.vgg import VGGNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "\n",
    "#env parameters\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters\n",
    "data_folder = Path(\"../../../audio/testfiles/GTZAN/genres/\")\n",
    "output_root = './../../../models/'\n",
    "output_folder = Path('./output/cvnn.model')\n",
    "output_model = output_root + '/cnn_dong_model_weights.h5'\n",
    "output_architecture = output_root + '/cnn_dong_model_architecture.json'\n",
    "output_whole = output_root + 'cnn_dong_model_whole.h5'\n",
    "output_label = output_root + 'label.pkl'\n",
    "output_test_paths = output_root + 'test_paths.pkl'\n",
    "spectogram_folder = Path(\"./img_data/\")\n",
    "# Duration of songsnippet in seconds\n",
    "duration = 2.97\n",
    "# Matplotlib colormap for spectogram\n",
    "spectogram_cmap = 'binary' \n",
    "# Predefined list of genres\n",
    "pred_genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split() \n",
    "epochs = 120\n",
    "\n",
    "# parameters\n",
    "sr = 22050 # if sampling rate is different, resample it to this\n",
    "\n",
    "# parameters for calculating spectrogram in mel scale\n",
    "fmax = 1500 # maximum frequency considered\n",
    "fft_window_points = 512\n",
    "fft_window_dur = fft_window_points * 1.0 / sr # 23ms windows\n",
    "hop_size = int(fft_window_points/ 2) # 50% overlap between consecutive frames\n",
    "n_mels = 64\n",
    "\n",
    "# segment duration\n",
    "num_fft_windows = 256 # num fft windows per music segment\n",
    "segment_in_points = num_fft_windows * 255 # number of data points that ensure the spectrogram has size: 64 * 256\n",
    "segment_dur = segment_in_points * 1.0 / sr\n",
    "\n",
    "num_genres=10\n",
    "input_shape=(64, 256, 1)\n",
    "\n",
    "randomseed = 11\n",
    "#randomseed = datetime.now()\n",
    "# Seed for RNG\n",
    "random.seed(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directories of all songs\n",
    "songs = []\n",
    "genres = []\n",
    "\n",
    "spectograms = []\n",
    "\n",
    "for g in data_folder.iterdir():\n",
    "    genres.append(g.name)\n",
    "    for i in g.iterdir():\n",
    "        songs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  1000\n",
      "Input shape: (1, 64, 256, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 1, 64, 256)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_specs = []  \n",
    "labels = []\n",
    "\n",
    "offset_1 = duration\n",
    "offset_2 = duration*2\n",
    "\n",
    "def load_specs(offset = 0):\n",
    "    offset = duration*offset\n",
    "    for song in songs:\n",
    "        y, sr = librosa.load(song, mono=True, offset=offset, duration=duration)\n",
    "        m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_window_points,\n",
    "                                              hop_length=hop_size, n_mels=n_mels,\n",
    "                                              fmax=fmax)\n",
    "        #if m_sp.shape != (128, 128): continue\n",
    "        mel_specs.append([m_sp])\n",
    "        label = song.parts[-2]\n",
    "        labels.append(label)\n",
    "        spectograms.append( (m_sp, label, song) )\n",
    "        input_shape = m_sp.shape + (1,)\n",
    "\n",
    "#data augmentation:\n",
    "def get_songs_num_offsets(num = 1):\n",
    "    for i  in range(num):\n",
    "        load_specs(offset = i)\n",
    "    return songs\n",
    "\n",
    "#get_songs_num_offsets(num = 4)\n",
    "load_specs(offset = 0)\n",
    "#load_specs(offset = 1)\n",
    "#load_specs(offset = 2)\n",
    "#load_specs(offset = 3)\n",
    "#load_specs(offset = 4)\n",
    "#load_specs(offset = 5)\n",
    "#load_specs(offset = 6)\n",
    "#load_specs(offset = 7)\n",
    "#load_specs(offset = 8)\n",
    "#load_specs(offset = 9)\n",
    "#spectograms = list(zip(mel_specs, labels))\n",
    "print(\"Total number of samples: \", len(mel_specs))\n",
    "\n",
    "#set dynamic input shape\n",
    "input_shape = np.shape(mel_specs[2]) + (1,)\n",
    "print(\"Input shape: \" + str(input_shape))\n",
    "np.shape(mel_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = input_shape[1] # use second dim as batch size, as it varies with duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectogram dimensions:  1 x 64\n"
     ]
    }
   ],
   "source": [
    "imagesize_x = np.shape(mel_specs)[1]\n",
    "imagesize_y = np.shape(mel_specs)[2]\n",
    "print(\"Spectogram dimensions: \", imagesize_x, \"x\", imagesize_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the spectograms\n",
    "random.shuffle(spectograms)\n",
    "#spectograms[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(334,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(334,)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: change\n",
    "testsplit = len(mel_specs)*2/3\n",
    "train = spectograms[:int(testsplit)]\n",
    "test = spectograms[int(testsplit):]\n",
    "\n",
    "X_train, y_train, p_train = zip(*train)\n",
    "X_test, y_test, p_test = zip(*test)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = np.array([x.reshape(input_shape) for x in X_train])\n",
    "X_test = np.array([x.reshape(input_shape) for x in X_test])\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "\n",
    "with open(output_label, 'wb') as f:\n",
    "    pickle.dump(lb, f)\n",
    "    \n",
    "with open(output_test_paths, 'wb') as f:\n",
    "    pickle.dump(p_test, f)\n",
    "\n",
    "\"\"\"\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(y_train)\n",
    "y_test_int = label_encoder.fit_transform(y_test)\n",
    "#print(integer_encoded)\n",
    "\n",
    "y_train_bin = np.array(keras.utils.to_categorical(y_train_int, 10))\n",
    "y_test_bin = np.array(keras.utils.to_categorical(y_test_int, 10))\n",
    "#print(\"Encoding of test data:\\n\", y_test, \"=>\\n\", y_test_int, \"=>\\n\", y_test_bin)\n",
    "\"\"\"\n",
    "0\n",
    "np.shape(p_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(655, 3)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_bin\n",
    "y_test = y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, shear_range=0.2, \n",
    "                         zoom_range=0.2,horizontal_flip=True, \n",
    "                         fill_mode=\"nearest\")\n",
    " \n",
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = VGGNet.build(width=imagesize_x, height=imagesize_y, depth=1, \n",
    "                          classes=len(lb.classes_))\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model for urban\n",
    "def cnn_urban_model_build():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model\n",
    "\n",
    "#model for GTZAN\n",
    "def cnn_dong_model_build():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 5), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_genres, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(decay=1e-6),\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_dong_model_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "H = model.fit(\n",
    "\tx=X_train, \n",
    "\ty=y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data= (X_test, y_test))\n",
    "\n",
    "score = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(X_test, batch_size=batch_size)\n",
    "print(classification_report(y_test_bin.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), \n",
    "                            target_names=lb.classes_))\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\"\"\"\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_bin.argmax(axis=1), predictions.argmax(axis=1))\n",
    "\n",
    "#cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "#model.save_weights(output_model)\n",
    "\n",
    "# Save the model architecture\n",
    "#with open(output_architecture, 'w') as f:\n",
    "#    f.write(model.to_json())\n",
    "    \n",
    "#model.save(output_whole)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

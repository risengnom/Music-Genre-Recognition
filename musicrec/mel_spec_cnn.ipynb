{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#TODO: cleanup, lint\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import librosa\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "    \n",
    "# import the necessary packages\n",
    "from musicrec.vgg import VGGNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters\n",
    "data_folder = Path(\"../../../audio/testfiles/GTZAN/genres/\")\n",
    "output_folder = Path(\"./output/cvnn.model\")\n",
    "spectogram_folder = Path(\"./img_data/\")\n",
    "# Duration of songsnippet in seconds\n",
    "duration = 2.97\n",
    "# Matplotlib colormap for spectogram\n",
    "spectogram_cmap = 'binary' \n",
    "# Predefined list of genres\n",
    "pred_genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split() \n",
    "epochs = 120\n",
    "\n",
    "# parameters\n",
    "sr = 22050 # if sampling rate is different, resample it to this\n",
    "\n",
    "# parameters for calculating spectrogram in mel scale\n",
    "fmax = 10000 # maximum frequency considered\n",
    "fft_window_points = 512\n",
    "fft_window_dur = fft_window_points * 1.0 / sr # 23ms windows\n",
    "hop_size = int(fft_window_points/ 2) # 50% overlap between consecutive frames\n",
    "n_mels = 64\n",
    "\n",
    "# segment duration\n",
    "num_fft_windows = 256 # num fft windows per music segment\n",
    "segment_in_points = num_fft_windows * 255 # number of data points that ensure the spectrogram has size: 64 * 256\n",
    "segment_dur = segment_in_points * 1.0 / sr\n",
    "\n",
    "num_genres=10\n",
    "input_shape=(64, 256, 1)\n",
    "\n",
    "randomseed = 11\n",
    "#randomseed = datetime.now()\n",
    "# Seed for RNG\n",
    "random.seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directories of all songs\n",
    "songs = []\n",
    "genres = []\n",
    "\n",
    "spectograms = []\n",
    "\n",
    "for g in data_folder.iterdir():\n",
    "    genres.append(g.name)\n",
    "    for i in g.iterdir():\n",
    "        songs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of songs:  4000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 256, 1)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_specs = []  \n",
    "labels = []\n",
    "spectograms = []\n",
    "\n",
    "offset_1 = duration\n",
    "offset_2 = duration*2\n",
    "\n",
    "def load_specs(offset = 0):\n",
    "    offset = duration*offset\n",
    "    for song in songs:\n",
    "        y, sr = librosa.load(song, mono=True, offset=offset, duration=duration)\n",
    "        m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_window_points,\n",
    "                                              hop_length=hop_size, n_mels=n_mels,\n",
    "                                              fmax=fmax)\n",
    "        #if m_sp.shape != (128, 128): continue\n",
    "        mel_specs.append(m_sp)\n",
    "        label = song.parts[-2]\n",
    "        labels.append(label)\n",
    "        spectograms.append( (m_sp, label) )\n",
    "\n",
    "#data augmentation:\n",
    "load_specs(offset = 0)\n",
    "load_specs(offset = 1)\n",
    "load_specs(offset = 2)\n",
    "load_specs(offset = 3)\n",
    "#spectograms = list(zip(mel_specs, labels))\n",
    "print(\"Total number of songs: \", len(mel_specs))\n",
    "input_shape = m_sp.shape + (1,)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = input_shape[1] # use second dim as batch size, as it varies with duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectogram dimensions:  64 x 256\n"
     ]
    }
   ],
   "source": [
    "imagesize_x = np.shape(mel_specs)[1]\n",
    "imagesize_y = np.shape(mel_specs)[2]\n",
    "print(\"Spectogram dimensions: \", imagesize_x, \"x\", imagesize_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the spectograms\n",
    "random.shuffle(spectograms)\n",
    "#spectograms[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: change\n",
    "testsplit = len(mel_specs)*2/3\n",
    "train = spectograms[:int(testsplit)]\n",
    "test = spectograms[int(testsplit):]\n",
    "\n",
    "X_train, y_train = zip(*train)\n",
    "X_test, y_test = zip(*test)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = np.array([x.reshape(input_shape) for x in X_train])\n",
    "X_test = np.array([x.reshape(input_shape) for x in X_test])\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(y_train)\n",
    "y_test_int = label_encoder.fit_transform(y_test)\n",
    "#print(integer_encoded)\n",
    "\n",
    "y_train_bin = np.array(keras.utils.to_categorical(y_train_int, 10))\n",
    "y_test_bin = np.array(keras.utils.to_categorical(y_test_int, 10))\n",
    "#print(\"Encoding of test data:\\n\", y_test, \"=>\\n\", y_test_int, \"=>\\n\", y_test_bin)\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_bin\n",
    "y_test = y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, shear_range=0.2, \n",
    "                         zoom_range=0.2,horizontal_flip=True, \n",
    "                         fill_mode=\"nearest\")\n",
    " \n",
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = VGGNet.build(width=imagesize_x, height=imagesize_y, depth=1, \n",
    "                          classes=len(lb.classes_))\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model for urban\n",
    "def cnn_urban_model_build()\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model\n",
    "\n",
    "#model for GTZAN\n",
    "def cnn_dong_model_build():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 5), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(num_genres, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(decay=1e-5),\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7f94a86fb6a0>>\n"
     ]
    }
   ],
   "source": [
    "model = cnn_dong_model_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2666 samples, validate on 1334 samples\n",
      "Epoch 1/120\n",
      "2666/2666 [==============================] - 83s 31ms/step - loss: 3.8690 - acc: 0.1339 - val_loss: 3.5415 - val_acc: 0.1357\n",
      "Epoch 2/120\n",
      "2666/2666 [==============================] - 84s 31ms/step - loss: 3.2171 - acc: 0.2183 - val_loss: 2.9320 - val_acc: 0.2489\n",
      "Epoch 3/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 2.8067 - acc: 0.2626 - val_loss: 2.6813 - val_acc: 0.2594\n",
      "Epoch 4/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 2.5769 - acc: 0.2817 - val_loss: 2.5537 - val_acc: 0.2684\n",
      "Epoch 5/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 2.3968 - acc: 0.3068 - val_loss: 2.4646 - val_acc: 0.2421\n",
      "Epoch 6/120\n",
      "2666/2666 [==============================] - 84s 31ms/step - loss: 2.2899 - acc: 0.3233 - val_loss: 2.4203 - val_acc: 0.2406\n",
      "Epoch 7/120\n",
      "2666/2666 [==============================] - 86s 32ms/step - loss: 2.2011 - acc: 0.3503 - val_loss: 2.1844 - val_acc: 0.3366\n",
      "Epoch 8/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 2.1176 - acc: 0.3657 - val_loss: 2.1384 - val_acc: 0.3756\n",
      "Epoch 9/120\n",
      "2666/2666 [==============================] - 84s 31ms/step - loss: 2.0445 - acc: 0.4089 - val_loss: 2.2669 - val_acc: 0.3163\n",
      "Epoch 10/120\n",
      "2666/2666 [==============================] - 85s 32ms/step - loss: 2.0222 - acc: 0.4096 - val_loss: 2.1363 - val_acc: 0.3966\n",
      "Epoch 11/120\n",
      "2666/2666 [==============================] - 88s 33ms/step - loss: 1.9285 - acc: 0.4400 - val_loss: 2.0943 - val_acc: 0.3643\n",
      "Epoch 12/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 1.8630 - acc: 0.4644 - val_loss: 2.0815 - val_acc: 0.3958\n",
      "Epoch 13/120\n",
      "2666/2666 [==============================] - 83s 31ms/step - loss: 1.9050 - acc: 0.4572 - val_loss: 2.1329 - val_acc: 0.3523\n",
      "Epoch 14/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.7962 - acc: 0.4812 - val_loss: 2.3771 - val_acc: 0.3238\n",
      "Epoch 15/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.8358 - acc: 0.4809 - val_loss: 2.2915 - val_acc: 0.3283\n",
      "Epoch 16/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.8238 - acc: 0.4771 - val_loss: 2.0514 - val_acc: 0.4220\n",
      "Epoch 17/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.6824 - acc: 0.5323 - val_loss: 2.1673 - val_acc: 0.3831\n",
      "Epoch 18/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.7795 - acc: 0.5019 - val_loss: 2.1416 - val_acc: 0.4250\n",
      "Epoch 19/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 1.6684 - acc: 0.5608 - val_loss: 2.0217 - val_acc: 0.4453\n",
      "Epoch 20/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.7242 - acc: 0.5405 - val_loss: 2.0639 - val_acc: 0.4198\n",
      "Epoch 21/120\n",
      "2666/2666 [==============================] - 85s 32ms/step - loss: 1.6017 - acc: 0.5720 - val_loss: 2.1794 - val_acc: 0.4070\n",
      "Epoch 22/120\n",
      "2666/2666 [==============================] - 86s 32ms/step - loss: 1.5592 - acc: 0.5855 - val_loss: 2.1447 - val_acc: 0.4205\n",
      "Epoch 23/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.6693 - acc: 0.5739 - val_loss: 2.1732 - val_acc: 0.4258\n",
      "Epoch 24/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 1.5598 - acc: 0.5859 - val_loss: 2.0755 - val_acc: 0.4460\n",
      "Epoch 25/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.6362 - acc: 0.5840 - val_loss: 2.1149 - val_acc: 0.4378\n",
      "Epoch 26/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.5122 - acc: 0.6193 - val_loss: 2.1152 - val_acc: 0.4558\n",
      "Epoch 27/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.3845 - acc: 0.6740 - val_loss: 2.4692 - val_acc: 0.3988\n",
      "Epoch 28/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 1.4319 - acc: 0.6530 - val_loss: 2.2341 - val_acc: 0.4130\n",
      "Epoch 29/120\n",
      "2666/2666 [==============================] - 88s 33ms/step - loss: 1.4347 - acc: 0.6440 - val_loss: 2.0688 - val_acc: 0.4588\n",
      "Epoch 30/120\n",
      "2666/2666 [==============================] - 89s 34ms/step - loss: 1.4691 - acc: 0.6309 - val_loss: 2.3571 - val_acc: 0.4115\n",
      "Epoch 31/120\n",
      "2666/2666 [==============================] - 86s 32ms/step - loss: 1.4432 - acc: 0.6564 - val_loss: 2.3829 - val_acc: 0.3943\n",
      "Epoch 32/120\n",
      "2666/2666 [==============================] - 89s 33ms/step - loss: 1.4304 - acc: 0.6553 - val_loss: 2.1433 - val_acc: 0.4805\n",
      "Epoch 33/120\n",
      "2666/2666 [==============================] - 84s 32ms/step - loss: 1.3506 - acc: 0.6962 - val_loss: 2.1558 - val_acc: 0.4685\n",
      "Epoch 34/120\n",
      "2666/2666 [==============================] - 85s 32ms/step - loss: 1.3275 - acc: 0.6887 - val_loss: 2.0599 - val_acc: 0.4895\n",
      "Epoch 35/120\n",
      "2666/2666 [==============================] - 88s 33ms/step - loss: 1.3045 - acc: 0.7217 - val_loss: 2.3783 - val_acc: 0.4475\n",
      "Epoch 36/120\n",
      "2666/2666 [==============================] - 87s 33ms/step - loss: 1.3083 - acc: 0.7041 - val_loss: 2.2361 - val_acc: 0.4663\n",
      "Epoch 37/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.3489 - acc: 0.6875 - val_loss: 2.2390 - val_acc: 0.4550\n",
      "Epoch 38/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.2718 - acc: 0.7056 - val_loss: 2.4429 - val_acc: 0.4100\n",
      "Epoch 39/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.3024 - acc: 0.7123 - val_loss: 2.4107 - val_acc: 0.4423\n",
      "Epoch 40/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 1.3134 - acc: 0.7089 - val_loss: 2.3288 - val_acc: 0.4108\n",
      "Epoch 41/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.2463 - acc: 0.7307 - val_loss: 2.3447 - val_acc: 0.4753\n",
      "Epoch 42/120\n",
      "2666/2666 [==============================] - 81s 31ms/step - loss: 1.2525 - acc: 0.7213 - val_loss: 2.4016 - val_acc: 0.4340\n",
      "Epoch 43/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.2744 - acc: 0.7209 - val_loss: 2.3854 - val_acc: 0.4250\n",
      "Epoch 44/120\n",
      "2666/2666 [==============================] - 81s 30ms/step - loss: 1.2394 - acc: 0.7419 - val_loss: 2.2302 - val_acc: 0.4588\n",
      "Epoch 45/120\n",
      "2666/2666 [==============================] - 87s 33ms/step - loss: 1.2744 - acc: 0.7191 - val_loss: 2.5535 - val_acc: 0.4295\n",
      "Epoch 46/120\n",
      "2666/2666 [==============================] - 83s 31ms/step - loss: 1.1954 - acc: 0.7543 - val_loss: 2.4289 - val_acc: 0.4453\n",
      "Epoch 47/120\n",
      "2666/2666 [==============================] - 89s 34ms/step - loss: 1.1650 - acc: 0.7584 - val_loss: 2.3418 - val_acc: 0.4460\n",
      "Epoch 48/120\n",
      "2666/2666 [==============================] - 87s 33ms/step - loss: 1.1294 - acc: 0.7686 - val_loss: 2.4395 - val_acc: 0.4663\n",
      "Epoch 49/120\n",
      "2666/2666 [==============================] - 85s 32ms/step - loss: 1.2201 - acc: 0.7517 - val_loss: 2.3420 - val_acc: 0.4715\n",
      "Epoch 50/120\n",
      "2666/2666 [==============================] - 90s 34ms/step - loss: 1.1426 - acc: 0.7719 - val_loss: 2.3929 - val_acc: 0.4475\n",
      "Epoch 51/120\n",
      "2666/2666 [==============================] - 91s 34ms/step - loss: 1.2383 - acc: 0.7224 - val_loss: 2.3753 - val_acc: 0.4393\n",
      "Epoch 52/120\n",
      "2666/2666 [==============================] - 93s 35ms/step - loss: 1.1465 - acc: 0.7637 - val_loss: 2.3168 - val_acc: 0.4783\n",
      "Epoch 53/120\n",
      "2666/2666 [==============================] - 82s 31ms/step - loss: 1.0999 - acc: 0.7806 - val_loss: 2.4102 - val_acc: 0.4798\n",
      "Epoch 54/120\n",
      "2666/2666 [==============================] - 84s 31ms/step - loss: 1.1741 - acc: 0.7558 - val_loss: 2.6654 - val_acc: 0.4340\n",
      "Epoch 55/120\n",
      "1536/2666 [================>.............] - ETA: 31s - loss: 1.1650 - acc: 0.7669"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "\tx=X_train, \n",
    "\ty=y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data= (X_test, y_test))\n",
    "\n",
    "score = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(X_test, batch_size=batch_size)\n",
    "print(classification_report(y_test_bin.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), \n",
    "                            target_names=lb.classes_))\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\"\"\"\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

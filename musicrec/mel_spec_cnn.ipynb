{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#TODO: cleanup, lint\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import librosa\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "    \n",
    "# import the necessary packages\n",
    "from musicrec.vgg import VGGNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "\n",
    "#env parameters\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters\n",
    "data_folder = Path(\"../../../audio/testfiles/GTZAN/genres/\")\n",
    "output_root = './../../../models/'\n",
    "output_folder = Path('./output/cvnn.model')\n",
    "output_model = output_root + '/cnn_dong_model_weights.h5'\n",
    "output_architecture = output_root + '/cnn_dong_model_architecture.json'\n",
    "output_whole = output_root + 'cnn_dong_model_whole.h5'\n",
    "output_label = output_root + 'label.pkl'\n",
    "spectogram_folder = Path(\"./img_data/\")\n",
    "# Duration of songsnippet in seconds\n",
    "duration = 2.97\n",
    "# Matplotlib colormap for spectogram\n",
    "spectogram_cmap = 'binary' \n",
    "# Predefined list of genres\n",
    "pred_genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split() \n",
    "epochs = 120\n",
    "\n",
    "# parameters\n",
    "sr = 22050 # if sampling rate is different, resample it to this\n",
    "\n",
    "# parameters for calculating spectrogram in mel scale\n",
    "fmax = 10000 # maximum frequency considered\n",
    "fft_window_points = 512\n",
    "fft_window_dur = fft_window_points * 1.0 / sr # 23ms windows\n",
    "hop_size = int(fft_window_points/ 2) # 50% overlap between consecutive frames\n",
    "n_mels = 64\n",
    "\n",
    "# segment duration\n",
    "num_fft_windows = 256 # num fft windows per music segment\n",
    "segment_in_points = num_fft_windows * 255 # number of data points that ensure the spectrogram has size: 64 * 256\n",
    "segment_dur = segment_in_points * 1.0 / sr\n",
    "\n",
    "num_genres=10\n",
    "input_shape=(64, 256, 1)\n",
    "\n",
    "randomseed = 11\n",
    "#randomseed = datetime.now()\n",
    "# Seed for RNG\n",
    "random.seed(datetime.now())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directories of all songs\n",
    "songs = []\n",
    "genres = []\n",
    "\n",
    "spectograms = []\n",
    "\n",
    "for g in data_folder.iterdir():\n",
    "    genres.append(g.name)\n",
    "    for i in g.iterdir():\n",
    "        songs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples:  1000\n",
      "Input shape: (64, 256, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1000, 64, 256)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_specs = []  \n",
    "labels = []\n",
    "spectograms = []\n",
    "\n",
    "offset_1 = duration\n",
    "offset_2 = duration*2\n",
    "\n",
    "def load_specs(offset = 0):\n",
    "    offset = duration*offset\n",
    "    for song in songs:\n",
    "        y, sr = librosa.load(song, mono=True, offset=offset, duration=duration)\n",
    "        m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_window_points,\n",
    "                                              hop_length=hop_size, n_mels=n_mels,\n",
    "                                              fmax=fmax)\n",
    "        #if m_sp.shape != (128, 128): continue\n",
    "        mel_specs.append(m_sp)\n",
    "        label = song.parts[-2]\n",
    "        labels.append(label)\n",
    "        spectograms.append( (m_sp, label) )\n",
    "        input_shape = m_sp.shape + (1,)\n",
    "\n",
    "#data augmentation:\n",
    "load_specs(offset = 0)\n",
    "#load_specs(offset = 1)\n",
    "#load_specs(offset = 2)\n",
    "#load_specs(offset = 3)\n",
    "#load_specs(offset = 4)\n",
    "#load_specs(offset = 5)\n",
    "#load_specs(offset = 6)\n",
    "#load_specs(offset = 7)\n",
    "#load_specs(offset = 8)\n",
    "#load_specs(offset = 9)\n",
    "#spectograms = list(zip(mel_specs, labels))\n",
    "print(\"Total number of samples: \", len(mel_specs))\n",
    "\n",
    "#set dynamic input shape\n",
    "input_shape = np.shape(mel_specs[2]) + (1,)\n",
    "print(\"Input shape: \" + str(input_shape))\n",
    "np.shape(mel_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = input_shape[1] # use second dim as batch size, as it varies with duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectogram dimensions:  64 x 256\n"
     ]
    }
   ],
   "source": [
    "imagesize_x = np.shape(mel_specs)[1]\n",
    "imagesize_y = np.shape(mel_specs)[2]\n",
    "print(\"Spectogram dimensions: \", imagesize_x, \"x\", imagesize_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the spectograms\n",
    "random.shuffle(spectograms)\n",
    "#spectograms[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(666, 64, 256, 1)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: change\n",
    "testsplit = len(mel_specs)*2/3\n",
    "train = spectograms[:int(testsplit)]\n",
    "test = spectograms[int(testsplit):]\n",
    "\n",
    "X_train, y_train = zip(*train)\n",
    "X_test, y_test = zip(*test)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = np.array([x.reshape(input_shape) for x in X_train])\n",
    "X_test = np.array([x.reshape(input_shape) for x in X_test])\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "\n",
    "with open(output_label, 'wb') as f:\n",
    "    pickle.dump(lb, f)\n",
    "\n",
    "\"\"\"\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(y_train)\n",
    "y_test_int = label_encoder.fit_transform(y_test)\n",
    "#print(integer_encoded)\n",
    "\n",
    "y_train_bin = np.array(keras.utils.to_categorical(y_train_int, 10))\n",
    "y_test_bin = np.array(keras.utils.to_categorical(y_test_int, 10))\n",
    "#print(\"Encoding of test data:\\n\", y_test, \"=>\\n\", y_test_int, \"=>\\n\", y_test_bin)\n",
    "\"\"\"\n",
    "0\n",
    "np.shape(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_bin\n",
    "y_test = y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, shear_range=0.2, \n",
    "                         zoom_range=0.2,horizontal_flip=True, \n",
    "                         fill_mode=\"nearest\")\n",
    " \n",
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = VGGNet.build(width=imagesize_x, height=imagesize_y, depth=1, \n",
    "                          classes=len(lb.classes_))\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model for urban\n",
    "def cnn_urban_model_build():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "    model.add(Activation('relu'))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(rate=0.5))\n",
    "\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=\"Adam\",\n",
    "        loss=\"categorical_crossentropy\",\n",
    "        metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model\n",
    "\n",
    "#model for GTZAN\n",
    "def cnn_dong_model_build():\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, kernel_size=(3, 3),\n",
    "                     activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 5), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_genres, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(decay=1e-5),\n",
    "                  metrics=['accuracy'])\n",
    "    print(model.summary)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Network.summary of <keras.engine.sequential.Sequential object at 0x7f09105385c0>>\n"
     ]
    }
   ],
   "source": [
    "model = cnn_dong_model_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666 samples, validate on 334 samples\n",
      "Epoch 1/120\n",
      "666/666 [==============================] - 22s 32ms/step - loss: 5.1559 - acc: 0.0796 - val_loss: 4.0646 - val_acc: 0.0988\n",
      "Epoch 2/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 4.1041 - acc: 0.0886 - val_loss: 3.8887 - val_acc: 0.0808\n",
      "Epoch 3/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 3.9142 - acc: 0.1186 - val_loss: 3.7561 - val_acc: 0.0838\n",
      "Epoch 4/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 3.7509 - acc: 0.1066 - val_loss: 3.6102 - val_acc: 0.0868\n",
      "Epoch 5/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 3.6167 - acc: 0.0886 - val_loss: 3.5245 - val_acc: 0.1407\n",
      "Epoch 6/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 3.5247 - acc: 0.1126 - val_loss: 3.4461 - val_acc: 0.0599\n",
      "Epoch 7/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 3.4353 - acc: 0.1111 - val_loss: 3.3720 - val_acc: 0.0749\n",
      "Epoch 8/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 3.3639 - acc: 0.1081 - val_loss: 3.3060 - val_acc: 0.0778\n",
      "Epoch 9/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 3.2912 - acc: 0.1126 - val_loss: 3.2465 - val_acc: 0.0719\n",
      "Epoch 10/120\n",
      "666/666 [==============================] - 22s 32ms/step - loss: 3.2296 - acc: 0.1111 - val_loss: 3.1843 - val_acc: 0.0659\n",
      "Epoch 11/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 3.1753 - acc: 0.1111 - val_loss: 3.1335 - val_acc: 0.0659\n",
      "Epoch 12/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 3.1335 - acc: 0.0961 - val_loss: 3.1084 - val_acc: 0.0778\n",
      "Epoch 13/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 3.0841 - acc: 0.1096 - val_loss: 3.0671 - val_acc: 0.0689\n",
      "Epoch 14/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 3.0412 - acc: 0.1291 - val_loss: 3.0225 - val_acc: 0.0749\n",
      "Epoch 15/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.9950 - acc: 0.1261 - val_loss: 2.9784 - val_acc: 0.0778\n",
      "Epoch 16/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.9883 - acc: 0.1216 - val_loss: 2.9329 - val_acc: 0.0689\n",
      "Epoch 17/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.9753 - acc: 0.1081 - val_loss: 2.9169 - val_acc: 0.1497\n",
      "Epoch 18/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.9110 - acc: 0.1321 - val_loss: 2.9010 - val_acc: 0.1647\n",
      "Epoch 19/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.9000 - acc: 0.1171 - val_loss: 2.8739 - val_acc: 0.1407\n",
      "Epoch 20/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.8616 - acc: 0.1216 - val_loss: 2.8394 - val_acc: 0.1257\n",
      "Epoch 21/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.8573 - acc: 0.1201 - val_loss: 2.8428 - val_acc: 0.1856\n",
      "Epoch 22/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.8379 - acc: 0.1291 - val_loss: 2.8162 - val_acc: 0.1766\n",
      "Epoch 23/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.7904 - acc: 0.1411 - val_loss: 2.7888 - val_acc: 0.1647\n",
      "Epoch 24/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.8038 - acc: 0.1231 - val_loss: 2.7760 - val_acc: 0.1317\n",
      "Epoch 25/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.7452 - acc: 0.1291 - val_loss: 2.7246 - val_acc: 0.1018\n",
      "Epoch 26/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.7590 - acc: 0.1562 - val_loss: 2.7153 - val_acc: 0.1677\n",
      "Epoch 27/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.6914 - acc: 0.1622 - val_loss: 2.6911 - val_acc: 0.1766\n",
      "Epoch 28/120\n",
      "666/666 [==============================] - 27s 40ms/step - loss: 2.7161 - acc: 0.1261 - val_loss: 2.6937 - val_acc: 0.1617\n",
      "Epoch 29/120\n",
      "666/666 [==============================] - 28s 42ms/step - loss: 2.6753 - acc: 0.1592 - val_loss: 2.6899 - val_acc: 0.1886\n",
      "Epoch 30/120\n",
      "666/666 [==============================] - 28s 42ms/step - loss: 2.6877 - acc: 0.1532 - val_loss: 2.6705 - val_acc: 0.1946\n",
      "Epoch 31/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.6755 - acc: 0.1547 - val_loss: 2.6412 - val_acc: 0.1766\n",
      "Epoch 32/120\n",
      "666/666 [==============================] - 26s 39ms/step - loss: 2.6338 - acc: 0.1441 - val_loss: 2.6181 - val_acc: 0.1976\n",
      "Epoch 33/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.6407 - acc: 0.1366 - val_loss: 2.5990 - val_acc: 0.2186\n",
      "Epoch 34/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.5990 - acc: 0.1517 - val_loss: 2.5802 - val_acc: 0.1856\n",
      "Epoch 35/120\n",
      "666/666 [==============================] - 28s 41ms/step - loss: 2.5955 - acc: 0.1607 - val_loss: 2.5652 - val_acc: 0.1976\n",
      "Epoch 36/120\n",
      "666/666 [==============================] - 28s 41ms/step - loss: 2.5706 - acc: 0.1622 - val_loss: 2.5579 - val_acc: 0.1976\n",
      "Epoch 37/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.5814 - acc: 0.1426 - val_loss: 2.5476 - val_acc: 0.2126\n",
      "Epoch 38/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.5486 - acc: 0.1532 - val_loss: 2.5247 - val_acc: 0.2126\n",
      "Epoch 39/120\n",
      "666/666 [==============================] - 28s 41ms/step - loss: 2.5498 - acc: 0.1441 - val_loss: 2.5182 - val_acc: 0.2186\n",
      "Epoch 40/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.4974 - acc: 0.1607 - val_loss: 2.4906 - val_acc: 0.2156\n",
      "Epoch 41/120\n",
      "666/666 [==============================] - 27s 41ms/step - loss: 2.5352 - acc: 0.1502 - val_loss: 2.4841 - val_acc: 0.2305\n",
      "Epoch 42/120\n",
      "666/666 [==============================] - 22s 33ms/step - loss: 2.5177 - acc: 0.1727 - val_loss: 2.4831 - val_acc: 0.2186\n",
      "Epoch 43/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.5002 - acc: 0.1532 - val_loss: 2.4763 - val_acc: 0.2665\n",
      "Epoch 44/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.4869 - acc: 0.1772 - val_loss: 2.4361 - val_acc: 0.2305\n",
      "Epoch 45/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.4781 - acc: 0.1532 - val_loss: 2.4219 - val_acc: 0.2425\n",
      "Epoch 46/120\n",
      "666/666 [==============================] - 22s 33ms/step - loss: 2.4446 - acc: 0.1727 - val_loss: 2.4289 - val_acc: 0.2186\n",
      "Epoch 47/120\n",
      "666/666 [==============================] - 22s 32ms/step - loss: 2.4429 - acc: 0.1847 - val_loss: 2.4094 - val_acc: 0.2305\n",
      "Epoch 48/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.4353 - acc: 0.1667 - val_loss: 2.4161 - val_acc: 0.2365\n",
      "Epoch 49/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.4588 - acc: 0.1532 - val_loss: 2.3955 - val_acc: 0.2156\n",
      "Epoch 50/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3997 - acc: 0.1922 - val_loss: 2.3842 - val_acc: 0.2605\n",
      "Epoch 51/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.4248 - acc: 0.1712 - val_loss: 2.3775 - val_acc: 0.2725\n",
      "Epoch 52/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3877 - acc: 0.1622 - val_loss: 2.3582 - val_acc: 0.2545\n",
      "Epoch 53/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.4204 - acc: 0.1862 - val_loss: 2.3815 - val_acc: 0.2874\n",
      "Epoch 54/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3918 - acc: 0.1967 - val_loss: 2.3699 - val_acc: 0.2695\n",
      "Epoch 55/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.3995 - acc: 0.1742 - val_loss: 2.3513 - val_acc: 0.2814\n",
      "Epoch 56/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3759 - acc: 0.2132 - val_loss: 2.3341 - val_acc: 0.2605\n",
      "Epoch 57/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3935 - acc: 0.1907 - val_loss: 2.3354 - val_acc: 0.2605\n",
      "Epoch 58/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3618 - acc: 0.2132 - val_loss: 2.3093 - val_acc: 0.2725\n",
      "Epoch 59/120\n",
      "666/666 [==============================] - 21s 32ms/step - loss: 2.3442 - acc: 0.2192 - val_loss: 2.2997 - val_acc: 0.2844\n",
      "Epoch 60/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3626 - acc: 0.2147 - val_loss: 2.3513 - val_acc: 0.2964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3751 - acc: 0.1772 - val_loss: 2.3327 - val_acc: 0.2934\n",
      "Epoch 62/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3000 - acc: 0.2132 - val_loss: 2.2890 - val_acc: 0.2874\n",
      "Epoch 63/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3253 - acc: 0.1937 - val_loss: 2.3166 - val_acc: 0.3383\n",
      "Epoch 64/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.2886 - acc: 0.2102 - val_loss: 2.2561 - val_acc: 0.3653\n",
      "Epoch 65/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3279 - acc: 0.2027 - val_loss: 2.2403 - val_acc: 0.3114\n",
      "Epoch 66/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3302 - acc: 0.2177 - val_loss: 2.2832 - val_acc: 0.2994\n",
      "Epoch 67/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3278 - acc: 0.2087 - val_loss: 2.2961 - val_acc: 0.2994\n",
      "Epoch 68/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.2843 - acc: 0.2102 - val_loss: 2.2741 - val_acc: 0.3293\n",
      "Epoch 69/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.3071 - acc: 0.2042 - val_loss: 2.2632 - val_acc: 0.2844\n",
      "Epoch 70/120\n",
      "666/666 [==============================] - 23s 34ms/step - loss: 2.3056 - acc: 0.2147 - val_loss: 2.2568 - val_acc: 0.2994\n",
      "Epoch 71/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.3028 - acc: 0.2207 - val_loss: 2.2660 - val_acc: 0.2934\n",
      "Epoch 72/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.2940 - acc: 0.2237 - val_loss: 2.2710 - val_acc: 0.3263\n",
      "Epoch 73/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.3011 - acc: 0.2087 - val_loss: 2.2300 - val_acc: 0.2814\n",
      "Epoch 74/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.2660 - acc: 0.2282 - val_loss: 2.2224 - val_acc: 0.2725\n",
      "Epoch 75/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.2117 - acc: 0.2432 - val_loss: 2.2522 - val_acc: 0.3174\n",
      "Epoch 76/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.2852 - acc: 0.2147 - val_loss: 2.2291 - val_acc: 0.3443\n",
      "Epoch 77/120\n",
      "666/666 [==============================] - 22s 33ms/step - loss: 2.2350 - acc: 0.2312 - val_loss: 2.2131 - val_acc: 0.3024\n",
      "Epoch 78/120\n",
      "666/666 [==============================] - 23s 35ms/step - loss: 2.2437 - acc: 0.2177 - val_loss: 2.2185 - val_acc: 0.3234\n",
      "Epoch 79/120\n",
      "666/666 [==============================] - 23s 35ms/step - loss: 2.2438 - acc: 0.2237 - val_loss: 2.2377 - val_acc: 0.2994\n",
      "Epoch 80/120\n",
      "666/666 [==============================] - 26s 39ms/step - loss: 2.2690 - acc: 0.2162 - val_loss: 2.2027 - val_acc: 0.2904\n",
      "Epoch 81/120\n",
      "666/666 [==============================] - 28s 42ms/step - loss: 2.1654 - acc: 0.2688 - val_loss: 2.2100 - val_acc: 0.3114\n",
      "Epoch 82/120\n",
      "666/666 [==============================] - 30s 45ms/step - loss: 2.2303 - acc: 0.2222 - val_loss: 2.2054 - val_acc: 0.3293\n",
      "Epoch 83/120\n",
      "666/666 [==============================] - 30s 45ms/step - loss: 2.2167 - acc: 0.2237 - val_loss: 2.2298 - val_acc: 0.3234\n",
      "Epoch 84/120\n",
      "666/666 [==============================] - 27s 40ms/step - loss: 2.2181 - acc: 0.2372 - val_loss: 2.1850 - val_acc: 0.3144\n",
      "Epoch 85/120\n",
      "666/666 [==============================] - 28s 42ms/step - loss: 2.2124 - acc: 0.2372 - val_loss: 2.1754 - val_acc: 0.2754\n",
      "Epoch 86/120\n",
      "666/666 [==============================] - 30s 45ms/step - loss: 2.2725 - acc: 0.2432 - val_loss: 2.2123 - val_acc: 0.3293\n",
      "Epoch 87/120\n",
      "666/666 [==============================] - 29s 44ms/step - loss: 2.2020 - acc: 0.2673 - val_loss: 2.1830 - val_acc: 0.3174\n",
      "Epoch 88/120\n",
      "666/666 [==============================] - 28s 42ms/step - loss: 2.2617 - acc: 0.2417 - val_loss: 2.1721 - val_acc: 0.2964\n",
      "Epoch 89/120\n",
      "666/666 [==============================] - 29s 44ms/step - loss: 2.2362 - acc: 0.2222 - val_loss: 2.1866 - val_acc: 0.3054\n",
      "Epoch 90/120\n",
      "666/666 [==============================] - 27s 40ms/step - loss: 2.1626 - acc: 0.2598 - val_loss: 2.1580 - val_acc: 0.3413\n",
      "Epoch 91/120\n",
      "666/666 [==============================] - 22s 34ms/step - loss: 2.1843 - acc: 0.2417 - val_loss: 2.1789 - val_acc: 0.3772\n",
      "Epoch 92/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1659 - acc: 0.2628 - val_loss: 2.2232 - val_acc: 0.3174\n",
      "Epoch 93/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.2186 - acc: 0.2447 - val_loss: 2.1611 - val_acc: 0.3563\n",
      "Epoch 94/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1940 - acc: 0.2718 - val_loss: 2.1816 - val_acc: 0.3323\n",
      "Epoch 95/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1778 - acc: 0.2447 - val_loss: 2.1840 - val_acc: 0.3293\n",
      "Epoch 96/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1599 - acc: 0.2808 - val_loss: 2.1684 - val_acc: 0.3293\n",
      "Epoch 97/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1906 - acc: 0.2628 - val_loss: 2.1604 - val_acc: 0.3503\n",
      "Epoch 98/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1654 - acc: 0.2553 - val_loss: 2.1708 - val_acc: 0.3234\n",
      "Epoch 99/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1737 - acc: 0.2823 - val_loss: 2.1519 - val_acc: 0.3234\n",
      "Epoch 100/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1511 - acc: 0.2628 - val_loss: 2.1699 - val_acc: 0.3084\n",
      "Epoch 101/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1419 - acc: 0.2913 - val_loss: 2.1252 - val_acc: 0.3353\n",
      "Epoch 102/120\n",
      "666/666 [==============================] - 27s 40ms/step - loss: 2.1580 - acc: 0.2763 - val_loss: 2.2168 - val_acc: 0.3084\n",
      "Epoch 103/120\n",
      "666/666 [==============================] - 27s 40ms/step - loss: 2.2022 - acc: 0.2688 - val_loss: 2.1527 - val_acc: 0.3473\n",
      "Epoch 104/120\n",
      "666/666 [==============================] - 26s 39ms/step - loss: 2.1309 - acc: 0.2928 - val_loss: 2.1615 - val_acc: 0.3174\n",
      "Epoch 105/120\n",
      "666/666 [==============================] - 23s 35ms/step - loss: 2.0941 - acc: 0.3138 - val_loss: 2.1551 - val_acc: 0.3353\n",
      "Epoch 106/120\n",
      "666/666 [==============================] - 23s 35ms/step - loss: 2.1787 - acc: 0.2523 - val_loss: 2.1793 - val_acc: 0.3383\n",
      "Epoch 107/120\n",
      "666/666 [==============================] - 23s 35ms/step - loss: 2.1126 - acc: 0.2748 - val_loss: 2.1324 - val_acc: 0.3503\n",
      "Epoch 108/120\n",
      "666/666 [==============================] - 23s 34ms/step - loss: 2.1002 - acc: 0.2718 - val_loss: 2.2410 - val_acc: 0.3263\n",
      "Epoch 109/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.2028 - acc: 0.2703 - val_loss: 2.1623 - val_acc: 0.3234\n",
      "Epoch 110/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.1325 - acc: 0.2823 - val_loss: 2.1498 - val_acc: 0.3353\n",
      "Epoch 111/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.0774 - acc: 0.2898 - val_loss: 2.1481 - val_acc: 0.3263\n",
      "Epoch 112/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.1191 - acc: 0.2868 - val_loss: 2.1479 - val_acc: 0.3473\n",
      "Epoch 113/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.1203 - acc: 0.2628 - val_loss: 2.1255 - val_acc: 0.3263\n",
      "Epoch 114/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.1148 - acc: 0.2688 - val_loss: 2.1302 - val_acc: 0.3383\n",
      "Epoch 115/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.0840 - acc: 0.3138 - val_loss: 2.1288 - val_acc: 0.3683\n",
      "Epoch 116/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.1085 - acc: 0.3033 - val_loss: 2.2853 - val_acc: 0.3024\n",
      "Epoch 117/120\n",
      "666/666 [==============================] - 20s 30ms/step - loss: 2.1776 - acc: 0.2733 - val_loss: 2.1403 - val_acc: 0.3623\n",
      "Epoch 118/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.0614 - acc: 0.3138 - val_loss: 2.1160 - val_acc: 0.3293\n",
      "Epoch 119/120\n",
      "666/666 [==============================] - 21s 31ms/step - loss: 2.1362 - acc: 0.3003 - val_loss: 2.1095 - val_acc: 0.3473\n",
      "Epoch 120/120\n",
      "666/666 [==============================] - 20s 31ms/step - loss: 2.1100 - acc: 0.3048 - val_loss: 2.1339 - val_acc: 0.3533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "334/334 [==============================] - 2s 7ms/step\n",
      "Test loss: 2.1339227053933514\n",
      "Test accuracy: 0.35329341370902373\n"
     ]
    }
   ],
   "source": [
    "H = model.fit(\n",
    "\tx=X_train, \n",
    "\ty=y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data= (X_test, y_test))\n",
    "\n",
    "score = model.evaluate(x=X_test,y=y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.37      0.31      0.34        35\n",
      "   classical       0.36      0.88      0.51        34\n",
      "     country       0.25      0.05      0.08        42\n",
      "       disco       0.33      0.63      0.44        27\n",
      "      hiphop       0.38      0.52      0.44        25\n",
      "        jazz       0.31      0.21      0.25        42\n",
      "       metal       0.47      0.26      0.34        34\n",
      "         pop       0.46      0.42      0.44        26\n",
      "      reggae       0.32      0.36      0.34        33\n",
      "        rock       0.24      0.11      0.15        36\n",
      "\n",
      "   micro avg       0.35      0.35      0.35       334\n",
      "   macro avg       0.35      0.38      0.33       334\n",
      "weighted avg       0.34      0.35      0.32       334\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(X_test, batch_size=batch_size)\n",
    "print(classification_report(y_test_bin.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), \n",
    "                            target_names=lb.classes_))\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\"\"\"\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "#model.save_weights(output_model)\n",
    "\n",
    "# Save the model architecture\n",
    "#with open(output_architecture, 'w') as f:\n",
    "#    f.write(model.to_json())\n",
    "    \n",
    "model.save(output_whole)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#TODO: cleanup, lint\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import librosa\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "    \n",
    "# import the necessary packages\n",
    "from musicrec.vgg import VGGNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"../../../audio/testfiles/GTZAN/genres/\")\n",
    "output_folder = Path(\"./output/cvnn.model\")\n",
    "spectogram_folder = Path(\"./img_data/\")\n",
    "# Duration of songsnippet in seconds\n",
    "duration = 2.97\n",
    "# Matplotlib colormap for spectogram\n",
    "spectogram_cmap = 'binary' \n",
    "# Predefined list of genres\n",
    "pred_genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split() \n",
    "epochs = 12\n",
    "\n",
    "randomseed = 11\n",
    "#randomseed = datetime.now()\n",
    "# Seed for RNG\n",
    "random.seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directories of all songs\n",
    "songs = []\n",
    "genres = []\n",
    "\n",
    "spectograms = []\n",
    "\n",
    "for g in data_folder.iterdir():\n",
    "    genres.append(g.name)\n",
    "    for i in g.iterdir():\n",
    "        songs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of songs:  1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "666.6666666666666"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_specs = []  \n",
    "labels = []\n",
    "spectograms = []\n",
    "for song in songs:\n",
    "    y, sr = librosa.load(song, mono=True, duration=duration)\n",
    "    m_sp = librosa.feature.melspectrogram(y=y, sr=sr)\n",
    "    #if m_sp.shape != (128, 128): continue\n",
    "    mel_specs.append(m_sp)\n",
    "    label = song.parts[-2]\n",
    "    labels.append(label)\n",
    "    spectograms.append( (m_sp, label) )\n",
    "#spectograms = list(zip(mel_specs, labels))\n",
    "print(\"Total number of songs: \", len(mel_specs))\n",
    "input_shape = m_sp.shape + (1,)\n",
    "len(mel_specs)*2/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = input_shape[1] # use second dim as batch size, as it varies with duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectogram dimensions:  128 x 128\n"
     ]
    }
   ],
   "source": [
    "imagesize_x = np.shape(mel_specs)[1]\n",
    "imagesize_y = np.shape(mel_specs)[2]\n",
    "print(\"Spectogram dimensions: \", imagesize_x, \"x\", imagesize_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the spectograms\n",
    "random.shuffle(spectograms)\n",
    "#spectograms[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: change\n",
    "testsplit = len(mel_specs)*2/3\n",
    "train = spectograms[:int(testsplit)]\n",
    "test = spectograms[int(testsplit):]\n",
    "\n",
    "X_train, y_train = zip(*train)\n",
    "X_test, y_test = zip(*test)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = np.array([x.reshape(input_shape) for x in X_train])\n",
    "X_test = np.array([x.reshape(input_shape) for x in X_test])\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(y_train)\n",
    "y_test_int = label_encoder.fit_transform(y_test)\n",
    "#print(integer_encoded)\n",
    "\n",
    "y_train_bin = np.array(keras.utils.to_categorical(y_train_int, 10))\n",
    "y_test_bin = np.array(keras.utils.to_categorical(y_test_int, 10))\n",
    "#print(\"Encoding of test data:\\n\", y_test, \"=>\\n\", y_test_int, \"=>\\n\", y_test_bin)\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_bin\n",
    "y_test = y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, shear_range=0.2, \n",
    "                         zoom_range=0.2,horizontal_flip=True, \n",
    "                         fill_mode=\"nearest\")\n",
    " \n",
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = VGGNet.build(width=imagesize_x, height=imagesize_y, depth=1, \n",
    "                          classes=len(lb.classes_))\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666 samples, validate on 334 samples\n",
      "Epoch 1/12\n",
      "666/666 [==============================] - 15s 22ms/step - loss: 8.3209 - acc: 0.0961 - val_loss: 3.0116 - val_acc: 0.1108\n",
      "Epoch 2/12\n",
      "666/666 [==============================] - 13s 20ms/step - loss: 4.0351 - acc: 0.1081 - val_loss: 2.3350 - val_acc: 0.1707\n",
      "Epoch 3/12\n",
      "666/666 [==============================] - 18s 26ms/step - loss: 2.8134 - acc: 0.1411 - val_loss: 2.3149 - val_acc: 0.1557\n",
      "Epoch 4/12\n",
      "666/666 [==============================] - 14s 21ms/step - loss: 2.4999 - acc: 0.1261 - val_loss: 2.2930 - val_acc: 0.1527\n",
      "Epoch 5/12\n",
      "666/666 [==============================] - 14s 22ms/step - loss: 2.3553 - acc: 0.1622 - val_loss: 2.2861 - val_acc: 0.1617\n",
      "Epoch 6/12\n",
      "666/666 [==============================] - 14s 22ms/step - loss: 2.3081 - acc: 0.1937 - val_loss: 2.2903 - val_acc: 0.1557\n",
      "Epoch 7/12\n",
      "666/666 [==============================] - 13s 20ms/step - loss: 2.2824 - acc: 0.1967 - val_loss: 2.2816 - val_acc: 0.1557\n",
      "Epoch 8/12\n",
      "666/666 [==============================] - 13s 20ms/step - loss: 2.2738 - acc: 0.1727 - val_loss: 2.2813 - val_acc: 0.1437\n",
      "Epoch 9/12\n",
      "666/666 [==============================] - 14s 21ms/step - loss: 2.2568 - acc: 0.1562 - val_loss: 2.2666 - val_acc: 0.1557\n",
      "Epoch 10/12\n",
      "666/666 [==============================] - 15s 22ms/step - loss: 2.2301 - acc: 0.1742 - val_loss: 2.2488 - val_acc: 0.1647\n",
      "Epoch 11/12\n",
      "666/666 [==============================] - 13s 20ms/step - loss: 2.2202 - acc: 0.1952 - val_loss: 2.2357 - val_acc: 0.1976\n",
      "Epoch 12/12\n",
      "666/666 [==============================] - 13s 20ms/step - loss: 2.1934 - acc: 0.1772 - val_loss: 2.2305 - val_acc: 0.1946\n",
      "334/334 [==============================] - 2s 7ms/step\n",
      "Test loss: 2.230458342386577\n",
      "Test accuracy: 0.1946107788000278\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "\toptimizer=\"Adam\",\n",
    "\tloss=\"categorical_crossentropy\",\n",
    "\tmetrics=['accuracy'])\n",
    "\n",
    "H = model.fit(\n",
    "\tx=X_train, \n",
    "\ty=y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data= (X_test, y_test_bin))\n",
    "\n",
    "score = model.evaluate(\n",
    "\tx=X_test,\n",
    "\ty=y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       blues       0.25      0.21      0.23        33\n",
      "   classical       0.40      0.61      0.48        28\n",
      "     country       0.14      0.28      0.19        29\n",
      "       disco       0.18      0.06      0.09        33\n",
      "      hiphop       0.21      0.32      0.26        37\n",
      "        jazz       0.43      0.08      0.13        39\n",
      "       metal       0.27      0.08      0.12        37\n",
      "         pop       0.23      0.29      0.25        31\n",
      "      reggae       0.06      0.14      0.08        28\n",
      "        rock       0.00      0.00      0.00        39\n",
      "\n",
      "   micro avg       0.19      0.19      0.19       334\n",
      "   macro avg       0.22      0.21      0.18       334\n",
      "weighted avg       0.22      0.19      0.18       334\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(X_test, batch_size=batch_size)\n",
    "print(classification_report(y_test_bin.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), \n",
    "                            target_names=lb.classes_))\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\"\"\"\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

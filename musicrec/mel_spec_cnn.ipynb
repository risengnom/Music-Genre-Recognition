{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the matplotlib backend so figures can be saved in the background\n",
    "#TODO: cleanup, lint\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    " \n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plot\n",
    "import librosa\n",
    "import keras\n",
    "from keras.layers import Activation, Dense, Dropout, Conv2D, \\\n",
    "                         Flatten, MaxPooling2D\n",
    "    \n",
    "# import the necessary packages\n",
    "from musicrec.vgg import VGGNet\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from numpy import argmax\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD\n",
    "from keras import regularizers\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.models import Sequential\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramters\n",
    "data_folder = Path(\"../../../audio/testfiles/GTZAN/genres/\")\n",
    "output_folder = Path(\"./output/cvnn.model\")\n",
    "spectogram_folder = Path(\"./img_data/\")\n",
    "# Duration of songsnippet in seconds\n",
    "duration = 2.97\n",
    "# Matplotlib colormap for spectogram\n",
    "spectogram_cmap = 'binary' \n",
    "# Predefined list of genres\n",
    "pred_genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split() \n",
    "epochs = 120\n",
    "\n",
    "# parameters\n",
    "sr = 22050 # if sampling rate is different, resample it to this\n",
    "\n",
    "# parameters for calculating spectrogram in mel scale\n",
    "fmax = 10000 # maximum frequency considered\n",
    "fft_window_points = 512\n",
    "fft_window_dur = fft_window_points * 1.0 / sr # 23ms windows\n",
    "hop_size = int(fft_window_points/ 2) # 50% overlap between consecutive frames\n",
    "n_mels = 64\n",
    "\n",
    "# segment duration\n",
    "num_fft_windows = 256 # num fft windows per music segment\n",
    "segment_in_points = num_fft_windows * 255 # number of data points that ensure the spectrogram has size: 64 * 256\n",
    "segment_dur = segment_in_points * 1.0 / sr\n",
    "\n",
    "num_genres=10\n",
    "input_shape=(64, 256, 1)\n",
    "\n",
    "randomseed = 11\n",
    "#randomseed = datetime.now()\n",
    "# Seed for RNG\n",
    "random.seed(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get directories of all songs\n",
    "songs = []\n",
    "genres = []\n",
    "\n",
    "spectograms = []\n",
    "\n",
    "for g in data_folder.iterdir():\n",
    "    genres.append(g.name)\n",
    "    for i in g.iterdir():\n",
    "        songs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of songs:  1000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64, 256, 1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mel_specs = []  \n",
    "labels = []\n",
    "spectograms = []\n",
    "for song in songs:\n",
    "    y, sr = librosa.load(song, mono=True, duration=duration)\n",
    "    m_sp = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=fft_window_points,\n",
    "                                          hop_length=hop_size, n_mels=n_mels,\n",
    "                                          fmax=fmax)\n",
    "    #if m_sp.shape != (128, 128): continue\n",
    "    mel_specs.append(m_sp)\n",
    "    label = song.parts[-2]\n",
    "    labels.append(label)\n",
    "    spectograms.append( (m_sp, label) )\n",
    "#spectograms = list(zip(mel_specs, labels))\n",
    "print(\"Total number of songs: \", len(mel_specs))\n",
    "input_shape = m_sp.shape + (1,)\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = input_shape[1] # use second dim as batch size, as it varies with duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spectogram dimensions:  64 x 256\n"
     ]
    }
   ],
   "source": [
    "imagesize_x = np.shape(mel_specs)[1]\n",
    "imagesize_y = np.shape(mel_specs)[2]\n",
    "print(\"Spectogram dimensions: \", imagesize_x, \"x\", imagesize_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the spectograms\n",
    "random.shuffle(spectograms)\n",
    "#spectograms[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO: change\n",
    "testsplit = len(mel_specs)*2/3\n",
    "train = spectograms[:int(testsplit)]\n",
    "test = spectograms[int(testsplit):]\n",
    "\n",
    "X_train, y_train = zip(*train)\n",
    "X_test, y_test = zip(*test)\n",
    "\n",
    "# Reshape for CNN input\n",
    "X_train = np.array([x.reshape(input_shape) for x in X_train])\n",
    "X_test = np.array([x.reshape(input_shape) for x in X_test])\n",
    "\n",
    "# One-Hot encoding for classes\n",
    "lb = LabelBinarizer()\n",
    "y_train_bin = lb.fit_transform(y_train)\n",
    "y_test_bin = lb.transform(y_test)\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_int = label_encoder.fit_transform(y_train)\n",
    "y_test_int = label_encoder.fit_transform(y_test)\n",
    "#print(integer_encoded)\n",
    "\n",
    "y_train_bin = np.array(keras.utils.to_categorical(y_train_int, 10))\n",
    "y_test_bin = np.array(keras.utils.to_categorical(y_test_int, 10))\n",
    "#print(\"Encoding of test data:\\n\", y_test, \"=>\\n\", y_test_int, \"=>\\n\", y_test_bin)\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train_bin\n",
    "y_test = y_test_bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "# construct the image generator for data augmentation\n",
    "aug = ImageDataGenerator(rotation_range=30, width_shift_range=0.1, \n",
    "                         height_shift_range=0.1, shear_range=0.2, \n",
    "                         zoom_range=0.2,horizontal_flip=True, \n",
    "                         fill_mode=\"nearest\")\n",
    " \n",
    "# initialize our VGG-like Convolutional Neural Network\n",
    "model = VGGNet.build(width=imagesize_x, height=imagesize_y, depth=1, \n",
    "                          classes=len(lb.classes_))\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old model for urban\n",
    "\"\"\"\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(24, (5, 5), strides=(1, 1), input_shape=input_shape))\n",
    "model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "model.add(MaxPooling2D((4, 2), strides=(4, 2)))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Conv2D(48, (5, 5), padding=\"valid\"))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(rate=0.5))\n",
    "\n",
    "model.add(Dense(10))\n",
    "model.add(Activation('softmax'))\n",
    "\"\"\"\n",
    "#model for GTZAN\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(96, kernel_size=(3, 3),\n",
    "                 activation='relu', kernel_regularizer=regularizers.l2(0.01),\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(64, (3, 5), activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 4)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.02)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(num_genres, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adadelta(decay=1e-5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 666 samples, validate on 334 samples\n",
      "Epoch 1/120\n",
      "666/666 [==============================] - 48s 72ms/step - loss: 4.5699 - acc: 0.1081 - val_loss: 4.1273 - val_acc: 0.1228\n",
      "Epoch 2/120\n",
      "666/666 [==============================] - 45s 68ms/step - loss: 4.0393 - acc: 0.1306 - val_loss: 3.8941 - val_acc: 0.1168\n",
      "Epoch 3/120\n",
      "666/666 [==============================] - 38s 57ms/step - loss: 3.8136 - acc: 0.2087 - val_loss: 3.7660 - val_acc: 0.1257\n",
      "Epoch 4/120\n",
      "666/666 [==============================] - 39s 58ms/step - loss: 3.6098 - acc: 0.1697 - val_loss: 3.6196 - val_acc: 0.1737\n",
      "Epoch 5/120\n",
      "666/666 [==============================] - 38s 57ms/step - loss: 3.4293 - acc: 0.2312 - val_loss: 3.5472 - val_acc: 0.1527\n",
      "Epoch 6/120\n",
      "666/666 [==============================] - 40s 60ms/step - loss: 3.3071 - acc: 0.2477 - val_loss: 3.3586 - val_acc: 0.2036\n",
      "Epoch 7/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 3.1627 - acc: 0.2673 - val_loss: 3.2487 - val_acc: 0.1946\n",
      "Epoch 8/120\n",
      "666/666 [==============================] - 40s 60ms/step - loss: 2.9878 - acc: 0.3063 - val_loss: 3.2084 - val_acc: 0.2096\n",
      "Epoch 9/120\n",
      "666/666 [==============================] - 38s 58ms/step - loss: 3.0454 - acc: 0.3033 - val_loss: 3.1374 - val_acc: 0.2246\n",
      "Epoch 10/120\n",
      "666/666 [==============================] - 38s 57ms/step - loss: 2.7943 - acc: 0.3694 - val_loss: 3.0079 - val_acc: 0.2246\n",
      "Epoch 11/120\n",
      "666/666 [==============================] - 38s 57ms/step - loss: 2.7076 - acc: 0.3243 - val_loss: 2.9606 - val_acc: 0.2365\n",
      "Epoch 12/120\n",
      "666/666 [==============================] - 39s 58ms/step - loss: 2.6282 - acc: 0.3694 - val_loss: 2.9301 - val_acc: 0.2455\n",
      "Epoch 13/120\n",
      "666/666 [==============================] - 39s 59ms/step - loss: 2.5452 - acc: 0.3964 - val_loss: 2.8888 - val_acc: 0.2216\n",
      "Epoch 14/120\n",
      "666/666 [==============================] - 39s 59ms/step - loss: 2.4738 - acc: 0.4054 - val_loss: 2.9521 - val_acc: 0.2425\n",
      "Epoch 15/120\n",
      "666/666 [==============================] - 39s 59ms/step - loss: 2.3625 - acc: 0.4324 - val_loss: 2.8042 - val_acc: 0.2695\n",
      "Epoch 16/120\n",
      "666/666 [==============================] - 41s 62ms/step - loss: 2.4138 - acc: 0.3979 - val_loss: 2.8000 - val_acc: 0.2425\n",
      "Epoch 17/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 2.2710 - acc: 0.4264 - val_loss: 3.0556 - val_acc: 0.2485\n",
      "Epoch 18/120\n",
      "666/666 [==============================] - 41s 62ms/step - loss: 2.2171 - acc: 0.4595 - val_loss: 2.7884 - val_acc: 0.2784\n",
      "Epoch 19/120\n",
      "666/666 [==============================] - 49s 73ms/step - loss: 2.0844 - acc: 0.4895 - val_loss: 2.8659 - val_acc: 0.2275\n",
      "Epoch 20/120\n",
      "666/666 [==============================] - 38s 58ms/step - loss: 2.0070 - acc: 0.5180 - val_loss: 2.8196 - val_acc: 0.2515\n",
      "Epoch 21/120\n",
      "666/666 [==============================] - 38s 56ms/step - loss: 2.2187 - acc: 0.4369 - val_loss: 2.8642 - val_acc: 0.2725\n",
      "Epoch 22/120\n",
      "666/666 [==============================] - 38s 57ms/step - loss: 2.0592 - acc: 0.5120 - val_loss: 2.8717 - val_acc: 0.2784\n",
      "Epoch 23/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.8971 - acc: 0.5556 - val_loss: 2.7704 - val_acc: 0.2934\n",
      "Epoch 24/120\n",
      "666/666 [==============================] - 41s 62ms/step - loss: 1.8103 - acc: 0.5991 - val_loss: 3.5543 - val_acc: 0.2515\n",
      "Epoch 25/120\n",
      "666/666 [==============================] - 41s 62ms/step - loss: 2.0245 - acc: 0.5150 - val_loss: 2.8680 - val_acc: 0.3114\n",
      "Epoch 26/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.6824 - acc: 0.6471 - val_loss: 3.5675 - val_acc: 0.2635\n",
      "Epoch 27/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 2.2326 - acc: 0.4640 - val_loss: 2.8135 - val_acc: 0.2994\n",
      "Epoch 28/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.6850 - acc: 0.6441 - val_loss: 3.0825 - val_acc: 0.3204\n",
      "Epoch 29/120\n",
      "666/666 [==============================] - 41s 62ms/step - loss: 1.7186 - acc: 0.6096 - val_loss: 3.0953 - val_acc: 0.2784\n",
      "Epoch 30/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.7241 - acc: 0.6306 - val_loss: 3.0211 - val_acc: 0.3114\n",
      "Epoch 31/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.4580 - acc: 0.7177 - val_loss: 3.1356 - val_acc: 0.3174\n",
      "Epoch 32/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.5228 - acc: 0.6817 - val_loss: 3.6278 - val_acc: 0.2754\n",
      "Epoch 33/120\n",
      "666/666 [==============================] - 42s 63ms/step - loss: 1.5761 - acc: 0.6592 - val_loss: 3.2170 - val_acc: 0.3263\n",
      "Epoch 34/120\n",
      "666/666 [==============================] - 43s 64ms/step - loss: 1.4789 - acc: 0.7042 - val_loss: 3.1898 - val_acc: 0.3174\n",
      "Epoch 35/120\n",
      "666/666 [==============================] - 43s 64ms/step - loss: 1.3502 - acc: 0.7402 - val_loss: 3.1832 - val_acc: 0.3443\n",
      "Epoch 36/120\n",
      "666/666 [==============================] - 50s 74ms/step - loss: 1.3126 - acc: 0.7553 - val_loss: 3.5386 - val_acc: 0.3054\n",
      "Epoch 37/120\n",
      "666/666 [==============================] - 48s 72ms/step - loss: 1.4362 - acc: 0.7057 - val_loss: 3.1514 - val_acc: 0.3563\n",
      "Epoch 38/120\n",
      "666/666 [==============================] - 44s 67ms/step - loss: 1.2671 - acc: 0.7898 - val_loss: 3.2110 - val_acc: 0.3353\n",
      "Epoch 39/120\n",
      "666/666 [==============================] - 43s 65ms/step - loss: 1.3037 - acc: 0.7492 - val_loss: 3.6783 - val_acc: 0.3024\n",
      "Epoch 40/120\n",
      "666/666 [==============================] - 40s 60ms/step - loss: 1.5222 - acc: 0.6742 - val_loss: 3.1931 - val_acc: 0.3174\n",
      "Epoch 41/120\n",
      "666/666 [==============================] - 47s 70ms/step - loss: 1.2545 - acc: 0.8078 - val_loss: 3.4231 - val_acc: 0.3353\n",
      "Epoch 42/120\n",
      "666/666 [==============================] - 48s 72ms/step - loss: 1.1631 - acc: 0.8108 - val_loss: 3.3033 - val_acc: 0.3413\n",
      "Epoch 43/120\n",
      "666/666 [==============================] - 51s 76ms/step - loss: 1.1309 - acc: 0.8243 - val_loss: 3.7050 - val_acc: 0.2994\n",
      "Epoch 44/120\n",
      "666/666 [==============================] - 50s 75ms/step - loss: 1.7363 - acc: 0.6532 - val_loss: 3.1107 - val_acc: 0.2994\n",
      "Epoch 45/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 1.3193 - acc: 0.7688 - val_loss: 3.1304 - val_acc: 0.3323\n",
      "Epoch 46/120\n",
      "666/666 [==============================] - 52s 78ms/step - loss: 1.1141 - acc: 0.8333 - val_loss: 3.3627 - val_acc: 0.3623\n",
      "Epoch 47/120\n",
      "666/666 [==============================] - 50s 75ms/step - loss: 1.0144 - acc: 0.8498 - val_loss: 3.5964 - val_acc: 0.3383\n",
      "Epoch 48/120\n",
      "666/666 [==============================] - 54s 82ms/step - loss: 1.1154 - acc: 0.8123 - val_loss: 3.9508 - val_acc: 0.3323\n",
      "Epoch 49/120\n",
      "666/666 [==============================] - 53s 79ms/step - loss: 1.2804 - acc: 0.7598 - val_loss: 3.2794 - val_acc: 0.3473\n",
      "Epoch 50/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 1.0458 - acc: 0.8408 - val_loss: 3.5683 - val_acc: 0.3413\n",
      "Epoch 51/120\n",
      "666/666 [==============================] - 51s 76ms/step - loss: 1.0136 - acc: 0.8619 - val_loss: 3.3919 - val_acc: 0.3503\n",
      "Epoch 52/120\n",
      "666/666 [==============================] - 51s 77ms/step - loss: 0.9607 - acc: 0.8679 - val_loss: 3.5573 - val_acc: 0.3623\n",
      "Epoch 53/120\n",
      "666/666 [==============================] - 50s 75ms/step - loss: 0.9524 - acc: 0.8619 - val_loss: 3.6635 - val_acc: 0.3503\n",
      "Epoch 54/120\n",
      "666/666 [==============================] - 50s 75ms/step - loss: 0.9791 - acc: 0.8288 - val_loss: 3.6252 - val_acc: 0.3443\n",
      "Epoch 55/120\n",
      "666/666 [==============================] - 50s 76ms/step - loss: 0.9859 - acc: 0.8559 - val_loss: 4.1816 - val_acc: 0.3293\n",
      "Epoch 56/120\n",
      "666/666 [==============================] - 51s 77ms/step - loss: 1.5408 - acc: 0.6832 - val_loss: 3.7534 - val_acc: 0.3024\n",
      "Epoch 57/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 1.1680 - acc: 0.8108 - val_loss: 3.3248 - val_acc: 0.3563\n",
      "Epoch 58/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.9435 - acc: 0.8739 - val_loss: 3.5229 - val_acc: 0.3563\n",
      "Epoch 59/120\n",
      "666/666 [==============================] - 49s 73ms/step - loss: 0.9061 - acc: 0.8784 - val_loss: 3.5521 - val_acc: 0.3473\n",
      "Epoch 60/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.8791 - acc: 0.8784 - val_loss: 3.7486 - val_acc: 0.3533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.8854 - acc: 0.8724 - val_loss: 3.6306 - val_acc: 0.3683\n",
      "Epoch 62/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.8725 - acc: 0.8784 - val_loss: 3.7604 - val_acc: 0.3353\n",
      "Epoch 63/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.8858 - acc: 0.8724 - val_loss: 3.9915 - val_acc: 0.3323\n",
      "Epoch 64/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.9094 - acc: 0.8634 - val_loss: 4.1443 - val_acc: 0.3114\n",
      "Epoch 65/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 1.0425 - acc: 0.8018 - val_loss: 3.8076 - val_acc: 0.3623\n",
      "Epoch 66/120\n",
      "666/666 [==============================] - 51s 76ms/step - loss: 0.8917 - acc: 0.8874 - val_loss: 3.7924 - val_acc: 0.3503\n",
      "Epoch 67/120\n",
      "666/666 [==============================] - 50s 76ms/step - loss: 0.8378 - acc: 0.8994 - val_loss: 3.9006 - val_acc: 0.3623\n",
      "Epoch 68/120\n",
      "666/666 [==============================] - 48s 72ms/step - loss: 0.8309 - acc: 0.8949 - val_loss: 3.9932 - val_acc: 0.3353\n",
      "Epoch 69/120\n",
      "666/666 [==============================] - 50s 75ms/step - loss: 0.8527 - acc: 0.8799 - val_loss: 3.9816 - val_acc: 0.3353\n",
      "Epoch 70/120\n",
      "666/666 [==============================] - 49s 73ms/step - loss: 0.9319 - acc: 0.8453 - val_loss: 3.8522 - val_acc: 0.3503\n",
      "Epoch 71/120\n",
      "666/666 [==============================] - 51s 77ms/step - loss: 0.8617 - acc: 0.8994 - val_loss: 4.0590 - val_acc: 0.3653\n",
      "Epoch 72/120\n",
      "666/666 [==============================] - 49s 74ms/step - loss: 0.9540 - acc: 0.8453 - val_loss: 4.4007 - val_acc: 0.3174\n",
      "Epoch 73/120\n",
      "666/666 [==============================] - 50s 75ms/step - loss: 1.1350 - acc: 0.7913 - val_loss: 3.4415 - val_acc: 0.3413\n",
      "Epoch 74/120\n",
      "666/666 [==============================] - 50s 74ms/step - loss: 0.8528 - acc: 0.8934 - val_loss: 3.7155 - val_acc: 0.3443\n",
      "Epoch 75/120\n",
      "512/666 [======================>.......] - ETA: 10s - loss: 0.7807 - acc: 0.9141"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-4e3ca82ad7c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     validation_data= (X_test, y_test))\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m score = model.evaluate(\n",
      "\u001b[0;32m~/.local/share/virtualenvs/is_music-genre-recognition-7WxnqNEu/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.local/share/virtualenvs/is_music-genre-recognition-7WxnqNEu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    210\u001b[0m                         val_outs = test_loop(model, val_f, val_ins,\n\u001b[1;32m    211\u001b[0m                                              \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m                                              verbose=0)\n\u001b[0m\u001b[1;32m    213\u001b[0m                         \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/is_music-genre-recognition-7WxnqNEu/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[1;32m    390\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/is_music-genre-recognition-7WxnqNEu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/is_music-genre-recognition-7WxnqNEu/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/is_music-genre-recognition-7WxnqNEu/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#model.compile(\n",
    "#\toptimizer=\"Adam\",\n",
    "#\tloss=\"categorical_crossentropy\",\n",
    "#\tmetrics=['accuracy'])\n",
    "\n",
    "H = model.fit(\n",
    "\tx=X_train, \n",
    "\ty=y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data= (X_test, y_test))\n",
    "\n",
    "score = model.evaluate(\n",
    "\tx=X_test,\n",
    "\ty=y_test)\n",
    "\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the network\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(X_test, batch_size=batch_size)\n",
    "print(classification_report(y_test_bin.argmax(axis=1),\n",
    "                            predictions.argmax(axis=1), \n",
    "                            target_names=lb.classes_))\n",
    " \n",
    "# plot the training loss and accuracy\n",
    "N = np.arange(0, epochs)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "\"\"\"\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"acc\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_acc\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy (SmallVGGNet)\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend()\n",
    "\"\"\"\n",
    "0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "musicrec2",
   "language": "python",
   "name": "musicrec2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "Currently using code taken from\n",
    "https://gist.github.com/parulnith/7f8c174e6ac099e86f0495d3d9a4c01e#file-music_genre_classification-ipynb\n",
    "for exploration of possible solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNnM2w-HCeb1"
   },
   "source": [
    "# Music genre classification notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l3sppZMCydR"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gt3fyg6dCNvX"
   },
   "outputs": [],
   "source": [
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPe_ebYuDqr5"
   },
   "source": [
    "## Extracting music and features\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use [GTZAN genre collection](http://marsyasweb.appspot.com/download/data_sets/) dataset for classification. \n",
    "<br>\n",
    "<br>\n",
    "The dataset consists of 10 genres i.e\n",
    " * Blues\n",
    " * Classical\n",
    " * Country\n",
    " * Disco\n",
    " * Hiphop\n",
    " * Jazz\n",
    " * Metal\n",
    " * Pop\n",
    " * Reggae\n",
    " * Rock\n",
    " \n",
    "Each genre contains 100 songs. Total dataset: 1000 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neqMS0VoDpN5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfBSVfRCD3PE"
   },
   "source": [
    "## Extracting the Spectrogram for every Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHh3pTEVDdrT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.get_cmap('binary')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'../../audio/testfiles/GTZAN/genres/{g}'):\n",
    "        songname = f'../../audio/testfiles/GTZAN/genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=5)\n",
    "        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
    "        plt.axis('off');\n",
    "        plt.savefig(f'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n",
    "        plt.clf()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SszVgjYnFNX9"
   },
   "source": [
    "All the audio files get converted into their respective spectrograms .WE can noe easily extract features from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Nw9HpSdFRsW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piwUwgP5Eef9"
   },
   "source": [
    "## Extracting features from Spectrogram\n",
    "\n",
    "\n",
    "We will extract\n",
    "\n",
    "* Mel-frequency cepstral coefficients (MFCC)(20 in number)\n",
    "* Spectral Centroid,\n",
    "* Zero Crossing Rate\n",
    "* Chroma Frequencies\n",
    "* Spectral Roll-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__g8tX8pDeIL"
   },
   "outputs": [],
   "source": [
    "header = 'filename chroma_stft spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBlT448pEqR9"
   },
   "source": [
    "## Writing data to csv file\n",
    "\n",
    "We write the data to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSQmB0PE3Iu"
   },
   "outputs": [],
   "source": [
    "file = open('data.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "filepath = '../../audio/testfiles/GTZAN/genres/'\n",
    "for g in genres:\n",
    "    for filename in os.listdir(f'{filepath}/{g}'):\n",
    "        songname = f'{filepath}/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        #rmse = librosa.feature.rmse(y=y, S=None, frame_length=2048, hop_length=512, center=True, pad_mode='reflect')\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        #to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g}'\n",
    "        file = open('data.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yfdo1cj6V7d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgeCZSKQEp1A"
   },
   "source": [
    "# Analysing the Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "Kr5_EdpD9dyh",
    "outputId": "81fd4a29-93fa-44f8-bf90-2f99981f761a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>mfcc3</th>\n",
       "      <th>mfcc4</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00085.wav</td>\n",
       "      <td>0.315363</td>\n",
       "      <td>1312.308199</td>\n",
       "      <td>1673.915613</td>\n",
       "      <td>2638.117381</td>\n",
       "      <td>0.059416</td>\n",
       "      <td>-180.123596</td>\n",
       "      <td>131.420259</td>\n",
       "      <td>0.566188</td>\n",
       "      <td>43.152929</td>\n",
       "      <td>...</td>\n",
       "      <td>2.718054</td>\n",
       "      <td>0.772589</td>\n",
       "      <td>1.997588</td>\n",
       "      <td>-6.093858</td>\n",
       "      <td>3.484042</td>\n",
       "      <td>-8.341479</td>\n",
       "      <td>3.204648</td>\n",
       "      <td>-0.926944</td>\n",
       "      <td>-2.243686</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00051.wav</td>\n",
       "      <td>0.393756</td>\n",
       "      <td>1977.172377</td>\n",
       "      <td>1927.803692</td>\n",
       "      <td>3942.834492</td>\n",
       "      <td>0.106627</td>\n",
       "      <td>-55.579243</td>\n",
       "      <td>114.935848</td>\n",
       "      <td>-37.052831</td>\n",
       "      <td>64.896513</td>\n",
       "      <td>...</td>\n",
       "      <td>12.782317</td>\n",
       "      <td>-16.528681</td>\n",
       "      <td>3.793788</td>\n",
       "      <td>-7.890870</td>\n",
       "      <td>8.477609</td>\n",
       "      <td>-4.065210</td>\n",
       "      <td>3.207441</td>\n",
       "      <td>-5.178250</td>\n",
       "      <td>-1.279524</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00038.wav</td>\n",
       "      <td>0.265883</td>\n",
       "      <td>1513.422107</td>\n",
       "      <td>2140.606779</td>\n",
       "      <td>3449.679140</td>\n",
       "      <td>0.044378</td>\n",
       "      <td>-192.667187</td>\n",
       "      <td>111.190774</td>\n",
       "      <td>21.372723</td>\n",
       "      <td>26.398341</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.044727</td>\n",
       "      <td>-17.631820</td>\n",
       "      <td>-7.921687</td>\n",
       "      <td>-15.152404</td>\n",
       "      <td>-12.342546</td>\n",
       "      <td>-17.227765</td>\n",
       "      <td>-7.771728</td>\n",
       "      <td>-13.033961</td>\n",
       "      <td>-17.945750</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00069.wav</td>\n",
       "      <td>0.291884</td>\n",
       "      <td>2371.099278</td>\n",
       "      <td>2209.699346</td>\n",
       "      <td>5004.111407</td>\n",
       "      <td>0.125141</td>\n",
       "      <td>-121.408845</td>\n",
       "      <td>96.106080</td>\n",
       "      <td>-19.613255</td>\n",
       "      <td>48.011654</td>\n",
       "      <td>...</td>\n",
       "      <td>6.793616</td>\n",
       "      <td>-19.333882</td>\n",
       "      <td>-0.114447</td>\n",
       "      <td>-12.250433</td>\n",
       "      <td>-4.927438</td>\n",
       "      <td>-10.409581</td>\n",
       "      <td>-2.049566</td>\n",
       "      <td>-1.768211</td>\n",
       "      <td>-3.962159</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00049.wav</td>\n",
       "      <td>0.277484</td>\n",
       "      <td>1318.656822</td>\n",
       "      <td>1904.761177</td>\n",
       "      <td>3046.681577</td>\n",
       "      <td>0.039973</td>\n",
       "      <td>-255.965774</td>\n",
       "      <td>110.823046</td>\n",
       "      <td>21.505215</td>\n",
       "      <td>67.571233</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.442929</td>\n",
       "      <td>-7.130865</td>\n",
       "      <td>-11.163917</td>\n",
       "      <td>-12.609408</td>\n",
       "      <td>2.742009</td>\n",
       "      <td>-8.948849</td>\n",
       "      <td>-7.940728</td>\n",
       "      <td>-2.344157</td>\n",
       "      <td>-0.705606</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename  chroma_stft  spectral_centroid  spectral_bandwidth  \\\n",
       "0  blues.00085.wav     0.315363        1312.308199         1673.915613   \n",
       "1  blues.00051.wav     0.393756        1977.172377         1927.803692   \n",
       "2  blues.00038.wav     0.265883        1513.422107         2140.606779   \n",
       "3  blues.00069.wav     0.291884        2371.099278         2209.699346   \n",
       "4  blues.00049.wav     0.277484        1318.656822         1904.761177   \n",
       "\n",
       "       rolloff  zero_crossing_rate       mfcc1       mfcc2      mfcc3  \\\n",
       "0  2638.117381            0.059416 -180.123596  131.420259   0.566188   \n",
       "1  3942.834492            0.106627  -55.579243  114.935848 -37.052831   \n",
       "2  3449.679140            0.044378 -192.667187  111.190774  21.372723   \n",
       "3  5004.111407            0.125141 -121.408845   96.106080 -19.613255   \n",
       "4  3046.681577            0.039973 -255.965774  110.823046  21.505215   \n",
       "\n",
       "       mfcc4  ...     mfcc12     mfcc13     mfcc14     mfcc15     mfcc16  \\\n",
       "0  43.152929  ...   2.718054   0.772589   1.997588  -6.093858   3.484042   \n",
       "1  64.896513  ...  12.782317 -16.528681   3.793788  -7.890870   8.477609   \n",
       "2  26.398341  ...  -5.044727 -17.631820  -7.921687 -15.152404 -12.342546   \n",
       "3  48.011654  ...   6.793616 -19.333882  -0.114447 -12.250433  -4.927438   \n",
       "4  67.571233  ...  -2.442929  -7.130865 -11.163917 -12.609408   2.742009   \n",
       "\n",
       "      mfcc17    mfcc18     mfcc19     mfcc20  label  \n",
       "0  -8.341479  3.204648  -0.926944  -2.243686  blues  \n",
       "1  -4.065210  3.207441  -5.178250  -1.279524  blues  \n",
       "2 -17.227765 -7.771728 -13.033961 -17.945750  blues  \n",
       "3 -10.409581 -2.049566  -1.768211  -3.962159  blues  \n",
       "4  -8.948849 -7.940728  -2.344157  -0.705606  blues  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iHrDHCaR9gKR",
    "outputId": "7d32943a-1ad5-4a59-c13a-beebeb36e4c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 27)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veD5BgX49hZa"
   },
   "outputs": [],
   "source": [
    "# Dropping unneccesary columns\n",
    "data = data.drop(['filename'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyr0aAAsGXjZ"
   },
   "source": [
    "## Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frI5HH4q-1HS"
   },
   "outputs": [],
   "source": [
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Slm8W0-iGVhI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2n8a02zGfvP"
   },
   "source": [
    "## Scaling the Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqcqn-nyAofk"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3VZvbwpGo9R"
   },
   "source": [
    "## Dividing data into training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1GW3VvQA7Rj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "upuczQ-KBHJ5",
    "outputId": "1431a28b-e8b6-4db2-e505-7e149e37c0d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LtoE_FqqBzM8",
    "outputId": "76555a2b-2030-48e1-b52d-d71b4ebae38e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ir9XaWgQB0lq",
    "outputId": "2ec90814-19d8-4f27-934a-1ce54406d4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.19983702, -1.12235633, -0.80675114, -0.98918641, -1.31311229,\n",
       "       -0.80344064,  0.76971867, -0.03839601,  1.21870608,  0.94871863,\n",
       "        0.50510579, -0.51532196,  0.52044259, -0.15009777,  0.63851434,\n",
       "       -0.43717433,  0.47832901, -0.85517149, -0.40676076, -0.28863192,\n",
       "       -0.86616406, -0.09871817,  0.2717664 , -0.72311185, -0.64936228])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vp2yc5FWG04e"
   },
   "source": [
    "# Classification with Keras\n",
    "\n",
    "## Building our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qj3sc2uFEUMt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/carsten/.local/share/virtualenvs/notebook-am4AQewu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yrsmpI6EjJ2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "bP0hVm4aElS7",
    "outputId": "aacf234d-d0a9-4de4-91be-5fd45a33b279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/carsten/.local/share/virtualenvs/notebook-am4AQewu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 467us/step - loss: 2.2002 - acc: 0.2337\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 1.8906 - acc: 0.3488\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 1.6508 - acc: 0.4200\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 1.4649 - acc: 0.4738\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 1.3206 - acc: 0.5413\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 1.2174 - acc: 0.5875\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 1.1268 - acc: 0.6112\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.0544 - acc: 0.6550\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.9912 - acc: 0.6687\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.9413 - acc: 0.6887\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.8875 - acc: 0.7112\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.8394 - acc: 0.7338\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.8004 - acc: 0.7388\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.7503 - acc: 0.7638\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.7207 - acc: 0.7612\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6826 - acc: 0.7850\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6503 - acc: 0.7900\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.6248 - acc: 0.8062\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.5990 - acc: 0.7925\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.5849 - acc: 0.8287\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.5471 - acc: 0.8275\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.5082 - acc: 0.8387\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.4995 - acc: 0.8475\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.4647 - acc: 0.8687\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.4577 - acc: 0.8700\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.4222 - acc: 0.8750\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.4149 - acc: 0.8750\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.4040 - acc: 0.8862\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.3757 - acc: 0.9025\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.3671 - acc: 0.8962\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.3455 - acc: 0.9037\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.3377 - acc: 0.9000\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.3113 - acc: 0.9163\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.2948 - acc: 0.9263\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.2788 - acc: 0.9337\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.2670 - acc: 0.9312\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.2543 - acc: 0.9438\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.2441 - acc: 0.9438\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.2332 - acc: 0.9363\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.2206 - acc: 0.9500\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.2042 - acc: 0.9637\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.1938 - acc: 0.9650\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.1922 - acc: 0.9625\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.1780 - acc: 0.9688\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1646 - acc: 0.9725\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1670 - acc: 0.9688\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1555 - acc: 0.9763\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.1401 - acc: 0.9838\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1367 - acc: 0.9788\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.1233 - acc: 0.9825\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1151 - acc: 0.9875\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1150 - acc: 0.9900\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.1071 - acc: 0.9913\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1026 - acc: 0.9888\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0963 - acc: 0.9888\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0900 - acc: 0.9950\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0908 - acc: 0.9925\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0820 - acc: 0.9962\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0774 - acc: 0.9962\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0716 - acc: 0.9975\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0673 - acc: 0.9975\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0658 - acc: 0.9962\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0609 - acc: 0.9975\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0581 - acc: 0.9962\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0578 - acc: 0.9975\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0564 - acc: 0.9962\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0524 - acc: 0.9975\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0472 - acc: 0.9975\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.0461 - acc: 0.9988\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0432 - acc: 0.9988\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0404 - acc: 0.9988\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0375 - acc: 0.9988\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0357 - acc: 0.9988\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0339 - acc: 0.9988\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0331 - acc: 0.9988\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0311 - acc: 0.9988\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0301 - acc: 0.9988\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0290 - acc: 0.9988\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0280 - acc: 0.9988\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.0268 - acc: 0.9988\n",
      "Epoch 81/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0254 - acc: 1.0000\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0258 - acc: 0.9988\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0235 - acc: 0.9988\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0219 - acc: 0.9988\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0215 - acc: 0.9988\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0211 - acc: 0.9988\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0203 - acc: 0.9988\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0196 - acc: 1.0000\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0192 - acc: 0.9988\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0187 - acc: 1.0000\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0180 - acc: 0.9988\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0169 - acc: 1.0000\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0166 - acc: 0.9988\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0185 - acc: 1.0000\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0188 - acc: 0.9988\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0158 - acc: 1.0000\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0142 - acc: 1.0000\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0137 - acc: 1.0000\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0135 - acc: 1.0000\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.0133 - acc: 1.0000\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0123 - acc: 1.0000\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0120 - acc: 1.0000\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0118 - acc: 0.9988\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0110 - acc: 1.0000\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0105 - acc: 1.0000\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0107 - acc: 1.0000\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0099 - acc: 1.0000\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0095 - acc: 1.0000\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0093 - acc: 1.0000\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0084 - acc: 1.0000\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0087 - acc: 1.0000\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0088 - acc: 1.0000\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0082 - acc: 1.0000\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0077 - acc: 1.0000\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0075 - acc: 1.0000\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0071 - acc: 1.0000\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0068 - acc: 1.0000\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0069 - acc: 1.0000\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0070 - acc: 1.0000\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0063 - acc: 1.0000\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0062 - acc: 1.0000\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0055 - acc: 1.0000\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0053 - acc: 1.0000\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.0052 - acc: 1.0000\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0051 - acc: 1.0000\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0049 - acc: 1.0000\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0047 - acc: 1.0000\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0046 - acc: 1.0000\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.0042 - acc: 1.0000\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0044 - acc: 1.0000\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0038 - acc: 1.0000\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0034 - acc: 1.0000\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0033 - acc: 1.0000\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 53us/step - loss: 0.0032 - acc: 1.0000\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 42us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0031 - acc: 1.0000\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 28us/step - loss: 0.0030 - acc: 1.0000\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0029 - acc: 1.0000\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0028 - acc: 1.0000\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0026 - acc: 1.0000\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0025 - acc: 1.0000\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 57us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 41us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0020 - acc: 1.0000\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.0018 - acc: 1.0000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=128)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0m1J0_wUFK4C",
    "outputId": "ffd3bf36-29ea-437a-987c-9aa600b9dae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 192us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6HrjXeUF0Ko",
    "outputId": "ea282dbd-6f9e-48c7-de2d-dc9afde8949e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc:  0.64\n"
     ]
    }
   ],
   "source": [
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yQmP_f5Kq0w"
   },
   "source": [
    "Tes accuracy is less than training dataa accuracy. This hints at Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-U2qzRJoHV9O"
   },
   "source": [
    "## Validating our approach\n",
    "Let's set apart 200 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJNbvYZoF7ZT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.7604279 ,  0.80956891, -0.13702939, ...,  0.66305118,\n",
       "        -0.89615763,  0.91367764],\n",
       "       [ 1.06577848,  1.09246308,  0.22651024, ...,  1.17337777,\n",
       "        -0.83382548,  2.535132  ],\n",
       "       [ 0.19108424,  1.00271175,  1.02043055, ...,  0.13309015,\n",
       "         0.12838833,  0.03948205],\n",
       "       ...,\n",
       "       [ 0.28626474, -0.00950845, -0.08346922, ...,  0.17329929,\n",
       "         0.27462279, -0.42498359],\n",
       "       [ 0.20785118,  0.32865694,  0.2287461 , ..., -0.31187917,\n",
       "        -0.13889837,  0.72759639],\n",
       "       [-0.10703749,  0.59479979,  0.29674725, ...,  0.77058291,\n",
       "        -0.70099519,  1.40212598]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val = X_train[:200]\n",
    "partial_x_train = X_train[200:]\n",
    "\n",
    "y_val = y_train[:200]\n",
    "partial_y_train = y_train[200:]\n",
    "x_norm = preprocessing.scale(x_val)\n",
    "x_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1EkG59EHeEV"
   },
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "Dp3G4P3aP4k2",
    "outputId": "25e1a389-1ac2-425b-bd5f-05736b6e9b96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "600/600 [==============================] - 0s 581us/step - loss: 2.2731 - acc: 0.1533 - val_loss: 2.1121 - val_acc: 0.3400\n",
      "Epoch 2/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 2.1232 - acc: 0.3300 - val_loss: 1.9617 - val_acc: 0.3650\n",
      "Epoch 3/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.9855 - acc: 0.3433 - val_loss: 1.8428 - val_acc: 0.3650\n",
      "Epoch 4/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 1.8531 - acc: 0.3617 - val_loss: 1.7407 - val_acc: 0.4050\n",
      "Epoch 5/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.7313 - acc: 0.3933 - val_loss: 1.6479 - val_acc: 0.4500\n",
      "Epoch 6/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 1.6093 - acc: 0.4367 - val_loss: 1.5531 - val_acc: 0.4700\n",
      "Epoch 7/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.4963 - acc: 0.5183 - val_loss: 1.4411 - val_acc: 0.4900\n",
      "Epoch 8/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 1.3905 - acc: 0.5600 - val_loss: 1.3370 - val_acc: 0.5200\n",
      "Epoch 9/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 1.2945 - acc: 0.5633 - val_loss: 1.2968 - val_acc: 0.5250\n",
      "Epoch 10/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 1.2176 - acc: 0.5950 - val_loss: 1.3008 - val_acc: 0.5250\n",
      "Epoch 11/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 1.1628 - acc: 0.6050 - val_loss: 1.2503 - val_acc: 0.5550\n",
      "Epoch 12/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 1.0925 - acc: 0.6317 - val_loss: 1.2258 - val_acc: 0.5450\n",
      "Epoch 13/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 1.0386 - acc: 0.6450 - val_loss: 1.2158 - val_acc: 0.5550\n",
      "Epoch 14/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.9914 - acc: 0.6750 - val_loss: 1.1745 - val_acc: 0.5850\n",
      "Epoch 15/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.9462 - acc: 0.6967 - val_loss: 1.1056 - val_acc: 0.5950\n",
      "Epoch 16/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.8993 - acc: 0.7000 - val_loss: 1.1147 - val_acc: 0.5850\n",
      "Epoch 17/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.8779 - acc: 0.7017 - val_loss: 1.1223 - val_acc: 0.5800\n",
      "Epoch 18/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.8189 - acc: 0.7333 - val_loss: 1.1087 - val_acc: 0.5800\n",
      "Epoch 19/200\n",
      "600/600 [==============================] - 0s 57us/step - loss: 0.7809 - acc: 0.7550 - val_loss: 1.0939 - val_acc: 0.6050\n",
      "Epoch 20/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.7658 - acc: 0.7567 - val_loss: 1.0856 - val_acc: 0.6050\n",
      "Epoch 21/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.7362 - acc: 0.7550 - val_loss: 1.0741 - val_acc: 0.5950\n",
      "Epoch 22/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.6845 - acc: 0.7733 - val_loss: 1.1165 - val_acc: 0.6000\n",
      "Epoch 23/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.6633 - acc: 0.7800 - val_loss: 1.0786 - val_acc: 0.6200\n",
      "Epoch 24/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.6428 - acc: 0.7783 - val_loss: 1.0107 - val_acc: 0.6250\n",
      "Epoch 25/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.6234 - acc: 0.7983 - val_loss: 1.0306 - val_acc: 0.6200\n",
      "Epoch 26/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.5829 - acc: 0.8267 - val_loss: 1.0957 - val_acc: 0.6000\n",
      "Epoch 27/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.5596 - acc: 0.8317 - val_loss: 1.0961 - val_acc: 0.6250\n",
      "Epoch 28/200\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.5404 - acc: 0.8233 - val_loss: 1.0796 - val_acc: 0.6300\n",
      "Epoch 29/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.5173 - acc: 0.8300 - val_loss: 1.0863 - val_acc: 0.6200\n",
      "Epoch 30/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.4914 - acc: 0.8583 - val_loss: 1.1190 - val_acc: 0.6050\n",
      "Epoch 31/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.4693 - acc: 0.8617 - val_loss: 1.1510 - val_acc: 0.6150\n",
      "Epoch 32/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.4549 - acc: 0.8650 - val_loss: 1.1566 - val_acc: 0.6250\n",
      "Epoch 33/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.4435 - acc: 0.8583 - val_loss: 1.1101 - val_acc: 0.6250\n",
      "Epoch 34/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.4231 - acc: 0.8783 - val_loss: 1.1174 - val_acc: 0.6500\n",
      "Epoch 35/200\n",
      "600/600 [==============================] - 0s 41us/step - loss: 0.4072 - acc: 0.8900 - val_loss: 1.1393 - val_acc: 0.6250\n",
      "Epoch 36/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.3779 - acc: 0.8950 - val_loss: 1.1258 - val_acc: 0.6400\n",
      "Epoch 37/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.3867 - acc: 0.8750 - val_loss: 1.1127 - val_acc: 0.6400\n",
      "Epoch 38/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.3543 - acc: 0.8850 - val_loss: 1.1501 - val_acc: 0.6400\n",
      "Epoch 39/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.3415 - acc: 0.9067 - val_loss: 1.1955 - val_acc: 0.6450\n",
      "Epoch 40/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.3416 - acc: 0.9017 - val_loss: 1.1955 - val_acc: 0.6400\n",
      "Epoch 41/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.3130 - acc: 0.9067 - val_loss: 1.1722 - val_acc: 0.6350\n",
      "Epoch 42/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.2962 - acc: 0.9200 - val_loss: 1.1556 - val_acc: 0.6400\n",
      "Epoch 43/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.2823 - acc: 0.9267 - val_loss: 1.1857 - val_acc: 0.6400\n",
      "Epoch 44/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.2727 - acc: 0.9300 - val_loss: 1.2182 - val_acc: 0.6150\n",
      "Epoch 45/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.2697 - acc: 0.9333 - val_loss: 1.1575 - val_acc: 0.6450\n",
      "Epoch 46/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.2498 - acc: 0.9483 - val_loss: 1.2095 - val_acc: 0.6300\n",
      "Epoch 47/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.2592 - acc: 0.9400 - val_loss: 1.2045 - val_acc: 0.6250\n",
      "Epoch 48/200\n",
      "600/600 [==============================] - 0s 53us/step - loss: 0.2349 - acc: 0.9500 - val_loss: 1.2458 - val_acc: 0.6450\n",
      "Epoch 49/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.2592 - acc: 0.9300 - val_loss: 1.3140 - val_acc: 0.6400\n",
      "Epoch 50/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.2264 - acc: 0.9483 - val_loss: 1.2709 - val_acc: 0.6500\n",
      "Epoch 51/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.2234 - acc: 0.9450 - val_loss: 1.2046 - val_acc: 0.6450\n",
      "Epoch 52/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.2314 - acc: 0.9383 - val_loss: 1.2618 - val_acc: 0.6350\n",
      "Epoch 53/200\n",
      "600/600 [==============================] - 0s 63us/step - loss: 0.2042 - acc: 0.9550 - val_loss: 1.3918 - val_acc: 0.6150\n",
      "Epoch 54/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.1910 - acc: 0.9583 - val_loss: 1.3569 - val_acc: 0.6400\n",
      "Epoch 55/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.1752 - acc: 0.9683 - val_loss: 1.2772 - val_acc: 0.6500\n",
      "Epoch 56/200\n",
      "600/600 [==============================] - 0s 54us/step - loss: 0.1791 - acc: 0.9617 - val_loss: 1.3201 - val_acc: 0.6400\n",
      "Epoch 57/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.1563 - acc: 0.9783 - val_loss: 1.4014 - val_acc: 0.6400\n",
      "Epoch 58/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1498 - acc: 0.9733 - val_loss: 1.3366 - val_acc: 0.6350\n",
      "Epoch 59/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.1341 - acc: 0.9817 - val_loss: 1.2721 - val_acc: 0.6400\n",
      "Epoch 60/200\n",
      "600/600 [==============================] - 0s 51us/step - loss: 0.1429 - acc: 0.9700 - val_loss: 1.3026 - val_acc: 0.6500\n",
      "Epoch 61/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1224 - acc: 0.9817 - val_loss: 1.3727 - val_acc: 0.6500\n",
      "Epoch 62/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1231 - acc: 0.9700 - val_loss: 1.3401 - val_acc: 0.6450\n",
      "Epoch 63/200\n",
      "600/600 [==============================] - 0s 41us/step - loss: 0.1133 - acc: 0.9850 - val_loss: 1.3219 - val_acc: 0.6400\n",
      "Epoch 64/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1039 - acc: 0.9900 - val_loss: 1.3637 - val_acc: 0.6350\n",
      "Epoch 65/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1035 - acc: 0.9900 - val_loss: 1.3710 - val_acc: 0.6500\n",
      "Epoch 66/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0963 - acc: 0.9917 - val_loss: 1.3860 - val_acc: 0.6500\n",
      "Epoch 67/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0911 - acc: 0.9917 - val_loss: 1.3740 - val_acc: 0.6500\n",
      "Epoch 68/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0872 - acc: 0.9933 - val_loss: 1.3685 - val_acc: 0.6400\n",
      "Epoch 69/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0887 - acc: 0.9933 - val_loss: 1.4277 - val_acc: 0.6350\n",
      "Epoch 70/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0819 - acc: 0.9950 - val_loss: 1.4688 - val_acc: 0.6550\n",
      "Epoch 71/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0760 - acc: 0.9967 - val_loss: 1.4477 - val_acc: 0.6550\n",
      "Epoch 72/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0734 - acc: 0.9933 - val_loss: 1.4382 - val_acc: 0.6500\n",
      "Epoch 73/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0675 - acc: 0.9950 - val_loss: 1.4802 - val_acc: 0.6450\n",
      "Epoch 74/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0666 - acc: 0.9967 - val_loss: 1.5029 - val_acc: 0.6350\n",
      "Epoch 75/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0601 - acc: 0.9983 - val_loss: 1.5015 - val_acc: 0.6700\n",
      "Epoch 76/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0573 - acc: 0.9983 - val_loss: 1.4790 - val_acc: 0.6650\n",
      "Epoch 77/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0558 - acc: 0.9967 - val_loss: 1.4586 - val_acc: 0.6600\n",
      "Epoch 78/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0520 - acc: 0.9983 - val_loss: 1.5019 - val_acc: 0.6450\n",
      "Epoch 79/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0510 - acc: 0.9967 - val_loss: 1.5638 - val_acc: 0.6400\n",
      "Epoch 80/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0472 - acc: 1.0000 - val_loss: 1.5871 - val_acc: 0.6500\n",
      "Epoch 81/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0476 - acc: 1.0000 - val_loss: 1.5272 - val_acc: 0.6450\n",
      "Epoch 82/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0425 - acc: 1.0000 - val_loss: 1.4918 - val_acc: 0.6550\n",
      "Epoch 83/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0438 - acc: 1.0000 - val_loss: 1.5451 - val_acc: 0.6300\n",
      "Epoch 84/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0398 - acc: 0.9967 - val_loss: 1.6486 - val_acc: 0.6450\n",
      "Epoch 85/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0390 - acc: 1.0000 - val_loss: 1.6625 - val_acc: 0.6450\n",
      "Epoch 86/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0361 - acc: 1.0000 - val_loss: 1.6023 - val_acc: 0.6500\n",
      "Epoch 87/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0330 - acc: 1.0000 - val_loss: 1.6034 - val_acc: 0.6350\n",
      "Epoch 88/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0308 - acc: 1.0000 - val_loss: 1.6587 - val_acc: 0.6450\n",
      "Epoch 89/200\n",
      "600/600 [==============================] - 0s 53us/step - loss: 0.0294 - acc: 1.0000 - val_loss: 1.6674 - val_acc: 0.6400\n",
      "Epoch 90/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0281 - acc: 1.0000 - val_loss: 1.6476 - val_acc: 0.6500\n",
      "Epoch 91/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0265 - acc: 1.0000 - val_loss: 1.6637 - val_acc: 0.6500\n",
      "Epoch 92/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 1.6928 - val_acc: 0.6500\n",
      "Epoch 93/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 1.7165 - val_acc: 0.6550\n",
      "Epoch 94/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0235 - acc: 1.0000 - val_loss: 1.7147 - val_acc: 0.6500\n",
      "Epoch 95/200\n",
      "600/600 [==============================] - 0s 40us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 1.6848 - val_acc: 0.6550\n",
      "Epoch 96/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0221 - acc: 1.0000 - val_loss: 1.6691 - val_acc: 0.6500\n",
      "Epoch 97/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0210 - acc: 1.0000 - val_loss: 1.6815 - val_acc: 0.6500\n",
      "Epoch 98/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0200 - acc: 1.0000 - val_loss: 1.7066 - val_acc: 0.6400\n",
      "Epoch 99/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0197 - acc: 1.0000 - val_loss: 1.7224 - val_acc: 0.6400\n",
      "Epoch 100/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 1.7339 - val_acc: 0.6400\n",
      "Epoch 101/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 1.7484 - val_acc: 0.6450\n",
      "Epoch 102/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 1.7584 - val_acc: 0.6350\n",
      "Epoch 103/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0172 - acc: 1.0000 - val_loss: 1.7702 - val_acc: 0.6400\n",
      "Epoch 104/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 1.7930 - val_acc: 0.6400\n",
      "Epoch 105/200\n",
      "600/600 [==============================] - 0s 41us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 1.7870 - val_acc: 0.6250\n",
      "Epoch 106/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0159 - acc: 1.0000 - val_loss: 1.7628 - val_acc: 0.6300\n",
      "Epoch 107/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 1.7642 - val_acc: 0.6200\n",
      "Epoch 108/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 1.7860 - val_acc: 0.6400\n",
      "Epoch 109/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 1.8197 - val_acc: 0.6400\n",
      "Epoch 110/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 1.8322 - val_acc: 0.6300\n",
      "Epoch 111/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0134 - acc: 1.0000 - val_loss: 1.8170 - val_acc: 0.6300\n",
      "Epoch 112/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 1.7998 - val_acc: 0.6300\n",
      "Epoch 113/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 1.7917 - val_acc: 0.6400\n",
      "Epoch 114/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 1.8109 - val_acc: 0.6400\n",
      "Epoch 115/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 1.8474 - val_acc: 0.6400\n",
      "Epoch 116/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 1.8592 - val_acc: 0.6500\n",
      "Epoch 117/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 1.8459 - val_acc: 0.6350\n",
      "Epoch 118/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 1.8282 - val_acc: 0.6400\n",
      "Epoch 119/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 1.8322 - val_acc: 0.6350\n",
      "Epoch 120/200\n",
      "600/600 [==============================] - 0s 55us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 1.8747 - val_acc: 0.6400\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 50us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 1.9302 - val_acc: 0.6350\n",
      "Epoch 122/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0107 - acc: 1.0000 - val_loss: 1.9371 - val_acc: 0.6450\n",
      "Epoch 123/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0105 - acc: 1.0000 - val_loss: 1.9000 - val_acc: 0.6450\n",
      "Epoch 124/200\n",
      "600/600 [==============================] - 0s 53us/step - loss: 0.0097 - acc: 1.0000 - val_loss: 1.8696 - val_acc: 0.6450\n",
      "Epoch 125/200\n",
      "600/600 [==============================] - 0s 53us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 1.8659 - val_acc: 0.6450\n",
      "Epoch 126/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 1.8887 - val_acc: 0.6450\n",
      "Epoch 127/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 1.9213 - val_acc: 0.6500\n",
      "Epoch 128/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 1.9413 - val_acc: 0.6550\n",
      "Epoch 129/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 1.9401 - val_acc: 0.6500\n",
      "Epoch 130/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 1.9257 - val_acc: 0.6450\n",
      "Epoch 131/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 1.9163 - val_acc: 0.6400\n",
      "Epoch 132/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 1.9132 - val_acc: 0.6400\n",
      "Epoch 133/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 1.9195 - val_acc: 0.6350\n",
      "Epoch 134/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 1.9318 - val_acc: 0.6400\n",
      "Epoch 135/200\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 1.9427 - val_acc: 0.6400\n",
      "Epoch 136/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 1.9524 - val_acc: 0.6400\n",
      "Epoch 137/200\n",
      "600/600 [==============================] - 0s 39us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 1.9673 - val_acc: 0.6400\n",
      "Epoch 138/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 1.9680 - val_acc: 0.6350\n",
      "Epoch 139/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 1.9506 - val_acc: 0.6350\n",
      "Epoch 140/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 1.9371 - val_acc: 0.6400\n",
      "Epoch 141/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 1.9353 - val_acc: 0.6450\n",
      "Epoch 142/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 1.9423 - val_acc: 0.6450\n",
      "Epoch 143/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 1.9612 - val_acc: 0.6450\n",
      "Epoch 144/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0062 - acc: 1.0000 - val_loss: 1.9836 - val_acc: 0.6450\n",
      "Epoch 145/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 1.9941 - val_acc: 0.6400\n",
      "Epoch 146/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 1.9922 - val_acc: 0.6450\n",
      "Epoch 147/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 1.9896 - val_acc: 0.6450\n",
      "Epoch 148/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 1.9871 - val_acc: 0.6500\n",
      "Epoch 149/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 1.9882 - val_acc: 0.6450\n",
      "Epoch 150/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 1.9989 - val_acc: 0.6350\n",
      "Epoch 151/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 2.0112 - val_acc: 0.6300\n",
      "Epoch 152/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 2.0238 - val_acc: 0.6350\n",
      "Epoch 153/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 2.0324 - val_acc: 0.6300\n",
      "Epoch 154/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 2.0374 - val_acc: 0.6350\n",
      "Epoch 155/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 2.0379 - val_acc: 0.6400\n",
      "Epoch 156/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 2.0249 - val_acc: 0.6400\n",
      "Epoch 157/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 2.0117 - val_acc: 0.6400\n",
      "Epoch 158/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.0073 - val_acc: 0.6400\n",
      "Epoch 159/200\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 2.0149 - val_acc: 0.6350\n",
      "Epoch 160/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 2.0262 - val_acc: 0.6400\n",
      "Epoch 161/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 2.0363 - val_acc: 0.6500\n",
      "Epoch 162/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 2.0496 - val_acc: 0.6500\n",
      "Epoch 163/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 2.0607 - val_acc: 0.6400\n",
      "Epoch 164/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 2.0638 - val_acc: 0.6350\n",
      "Epoch 165/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0044 - acc: 1.0000 - val_loss: 2.0639 - val_acc: 0.6350\n",
      "Epoch 166/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0043 - acc: 1.0000 - val_loss: 2.0642 - val_acc: 0.6350\n",
      "Epoch 167/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0042 - acc: 1.0000 - val_loss: 2.0695 - val_acc: 0.6400\n",
      "Epoch 168/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0041 - acc: 1.0000 - val_loss: 2.0753 - val_acc: 0.6400\n",
      "Epoch 169/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 2.0740 - val_acc: 0.6400\n",
      "Epoch 170/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0040 - acc: 1.0000 - val_loss: 2.0657 - val_acc: 0.6350\n",
      "Epoch 171/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 2.0592 - val_acc: 0.6400\n",
      "Epoch 172/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0039 - acc: 1.0000 - val_loss: 2.0601 - val_acc: 0.6400\n",
      "Epoch 173/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.0695 - val_acc: 0.6450\n",
      "Epoch 174/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0038 - acc: 1.0000 - val_loss: 2.0857 - val_acc: 0.6450\n",
      "Epoch 175/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0037 - acc: 1.0000 - val_loss: 2.0980 - val_acc: 0.6450\n",
      "Epoch 176/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 2.0990 - val_acc: 0.6400\n",
      "Epoch 177/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0036 - acc: 1.0000 - val_loss: 2.0896 - val_acc: 0.6400\n",
      "Epoch 178/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.0819 - val_acc: 0.6400\n",
      "Epoch 179/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.0839 - val_acc: 0.6400\n",
      "Epoch 180/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0035 - acc: 1.0000 - val_loss: 2.0909 - val_acc: 0.6400\n",
      "Epoch 181/200\n",
      "600/600 [==============================] - 0s 55us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 2.0993 - val_acc: 0.6400\n",
      "Epoch 182/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0034 - acc: 1.0000 - val_loss: 2.1078 - val_acc: 0.6400\n",
      "Epoch 183/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 2.1164 - val_acc: 0.6350\n",
      "Epoch 184/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0033 - acc: 1.0000 - val_loss: 2.1224 - val_acc: 0.6450\n",
      "Epoch 185/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 2.1292 - val_acc: 0.6450\n",
      "Epoch 186/200\n",
      "600/600 [==============================] - 0s 41us/step - loss: 0.0032 - acc: 1.0000 - val_loss: 2.1319 - val_acc: 0.6450\n",
      "Epoch 187/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.1268 - val_acc: 0.6400\n",
      "Epoch 188/200\n",
      "600/600 [==============================] - 0s 68us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.1183 - val_acc: 0.6400\n",
      "Epoch 189/200\n",
      "600/600 [==============================] - 0s 53us/step - loss: 0.0031 - acc: 1.0000 - val_loss: 2.1136 - val_acc: 0.6400\n",
      "Epoch 190/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.1155 - val_acc: 0.6400\n",
      "Epoch 191/200\n",
      "600/600 [==============================] - 0s 51us/step - loss: 0.0030 - acc: 1.0000 - val_loss: 2.1235 - val_acc: 0.6400\n",
      "Epoch 192/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1375 - val_acc: 0.6400\n",
      "Epoch 193/200\n",
      "600/600 [==============================] - 0s 58us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1488 - val_acc: 0.6400\n",
      "Epoch 194/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1502 - val_acc: 0.6400\n",
      "Epoch 195/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 2.1490 - val_acc: 0.6450\n",
      "Epoch 196/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.1506 - val_acc: 0.6450\n",
      "Epoch 197/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0028 - acc: 1.0000 - val_loss: 2.1552 - val_acc: 0.6500\n",
      "Epoch 198/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.1606 - val_acc: 0.6400\n",
      "Epoch 199/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.1684 - val_acc: 0.6400\n",
      "Epoch 200/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0027 - acc: 1.0000 - val_loss: 2.1751 - val_acc: 0.6450\n",
      "200/200 [==============================] - 0s 54us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=200,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dljqHfDPI6lH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mvi9it1SI4aR",
    "outputId": "98b01ef2-3935-442b-82d6-45f56e036d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.341206932067871, 0.605]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3hb8s1l4rBA"
   },
   "source": [
    "## Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gudBAhIXJIi2"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xb7bVPSwJQF0",
    "outputId": "aca09c75-1d21-4847-bdd9-a0521dc8d948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "llusRQV0JRy9",
    "outputId": "a856289d-883a-47cb-c0fb-ec148330a60a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0eoEuSZqJTdU",
    "outputId": "94c17d00-dd7f-40a1-84d2-78d1ebde6103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Utgt1bXfJVRN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled9.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import copy\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        #parameters for Adam optimizer\n",
    "        self.vdb = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.vdw = [np.zeros(w.shape) for w in self.weights]\n",
    "        self.sdb = [np.zeros(b.shape) for b in self.biases]\n",
    "        self.sdw = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def feedforward(self, a,using_soft_max=False):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            a = sigmoid(np.dot(w, a)+ b)\n",
    "            \n",
    "        b,w = (self.biases[-1],self.weights[-1])\n",
    "        if using_soft_max:\n",
    "            a = soft_max(np.dot(w, a)+ b)\n",
    "        else:\n",
    "            a = sigmoid(np.dot(w, a)+ b)\n",
    "        return a\n",
    "    \n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,test_data=None,using_soft_max = False,using_adam_optimizer = False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The ``training_data`` is a list of tuples\n",
    "        ``(x, y)`` representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If ``test_data`` is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "        if test_data != None:\n",
    "            n_test = len(test_data)\n",
    "\n",
    "        n = len(training_data)\n",
    "        fail=0\n",
    "        maximum=0\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta, j, using_soft_max=using_soft_max,using_adam_optimizer=using_adam_optimizer)\n",
    "            if test_data != None:\n",
    "                \n",
    "                actual=self.evaluate(test_data,using_soft_max)\n",
    "                #print(\"actual {0} max {1}\".format(actual,maximum))\n",
    "                if (actual/n_test)>maximum:\n",
    "                    maximum=actual/n_test\n",
    "                    fail=0\n",
    "                    network=copy.deepcopy(self)\n",
    "                else:\n",
    "                    fail+=1\n",
    "                print(\"Epoch {0}: {1} / {2}, {3}%\".format(j, actual, n_test,actual/n_test))\n",
    "                print(fail)\n",
    "                if fail==20:\n",
    "                    break\n",
    "            else:\n",
    "                print(\"Epoch {0} complete\".format(j))\n",
    "        network.saveWeightsAndBiases(\"Network2\")\n",
    "    \n",
    "    def update_mini_batch(self, mini_batch, eta,epoch,using_soft_max=False,using_adam_optimizer = False):\n",
    "        beta1 = 0.9 \n",
    "        beta2 = 0.999\n",
    "        epsilon = np.array([pow(10, -8)])\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The ``mini_batch`` is a list of tuples ``(x, y)``, and ``eta``\n",
    "        is the learning rate.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y, soft_max)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        #Adam optimizer\n",
    "        if using_adam_optimizer:\n",
    "            self.vdw = np.array([beta1*vv+(1-beta1)*nw for vv, nw in zip(self.vdw,nabla_w)])\n",
    "            self.vdb = np.array([beta1*bb+(1-beta1)*nb for bb, nb in zip(self.vdb,nabla_b)])\n",
    "            self.sdw = np.array([beta2*ss+(1-beta2)*pow(nw, 2) for ss, nw in zip(self.sdw,nabla_w)])\n",
    "            self.sdb = np.array([beta2*ss+(1-beta2)*pow(nw, 2) for ss, nw in zip(self.sdb,nabla_b)])\n",
    "            vdw_corrected = self.vdw / (1-pow(beta1, epoch+1))\n",
    "            vdb_corrected = self.vdb / (1-pow(beta1, epoch+1))\n",
    "            sdw_corrected = self.sdw / (1-pow(beta2,epoch+1))\n",
    "            sdb_corrected = self.sdb / (1-pow(beta2,epoch+1))\n",
    "            self.weights = [w - (eta * (v_corrected / (np.sqrt(s_corrected)+ epsilon)))\n",
    "                        for w, v_corrected, s_corrected in zip(self.weights,vdw_corrected,sdw_corrected)]\n",
    "            self.biases = [b - (eta * (v_corrected / (np.sqrt(s_corrected)+ epsilon)))\n",
    "                        for b, v_corrected, s_corrected in zip(self.biases,vdb_corrected,sdb_corrected)]\n",
    "        else:     \n",
    "            self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "            self.biases = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.biases, nabla_b)]\n",
    "    \n",
    "    def backprop(self, x, y, using_soft_max=False,using_cross_entropy = True):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases[:-1], self.weights[:-1]):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        w,b = (self.weights[-1],self.biases[-1])\n",
    "        z = np.dot(w, activation)+b\n",
    "        zs.append(z)\n",
    "        if using_soft_max:\n",
    "            activation = soft_max(z)\n",
    "        else:\n",
    "            activation = sigmoid(z)\n",
    "        activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * soft_max_prime(zs[-1])\n",
    "        if using_cross_entropy:\n",
    "            delta = self.cost_derivative(activations[-1], y)\n",
    "        else:\n",
    "            delta = self.cost_derivative(activations[-1], y) * soft_max_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "    \n",
    "    def evaluate(self, test_data, soft_max):\n",
    "        \"\"\"Return the number of test inputs for which the neural\n",
    "        network outputs the correct result. Note that the neural\n",
    "        network's output is assumed to be the index of whichever\n",
    "        neuron in the final layer has the highest activation.\"\"\"\n",
    "        test_results = [(np.argmax(self.feedforward(x,using_soft_max=soft_max)), np.argmax(y)) for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "    \n",
    "    def saveWeightsAndBiases(self,fileName):\n",
    "        np.savez(fileName,self.weights,self.biases)\n",
    "    \n",
    "    def loadWeightsAndBiases(self,fileName):\n",
    "        npzfile = np.load(fileName)\n",
    "        self.weights = npzfile['arr_0']\n",
    "        self.biases = npzfile['arr_1']\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(z):\n",
    "    expA = np.exp(z - np.max(z))\n",
    "    res = expA / expA.sum()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max_prime(z):\n",
    "    return soft_max(z)*(1-soft_max(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluDerivative(x):\n",
    "    x[x<=0] = 0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reluprime(x):\n",
    "    return np.where(x > 0, 1.0, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marti\\Desktop\\Universidad\\Suecia\\Int sys\\DE\n"
     ]
    }
   ],
   "source": [
    "%cd C:\\Users\\marti\\Desktop\\Universidad\\Suecia\\Int sys\\DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [],
   "source": [
    "supported_genres=['blues','classical','country','disco','hiphop','jazz','metal','pop','reggae','rock']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalising all features\n",
    "data = pd.read_csv('predictions_cnn_19k.csv')\n",
    "genre_list=[]\n",
    "\n",
    "for row in  data.loc[:,'filename']:\n",
    "    genre_list.append(row.split('.')[0])\n",
    "\n",
    "minMaxScaler = MinMaxScaler()\n",
    "X=minMaxScaler.fit_transform(np.array(data.iloc[:,3:-10], dtype = float))\n",
    "\n",
    "normalised_values_table = pd.concat([data.loc[:,['filename','offset','duration']],\n",
    "                           pd.DataFrame(X,columns=data.columns[3:-10]),\n",
    "                           data.loc[:,supported_genres],\n",
    "                           pd.DataFrame(np.array(genre_list),columns=['genre'])],\n",
    "                           axis=1)\n",
    "normalised_values_table.to_csv('Normalised_Features.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating desired output according to genre column in table\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)\n",
    "auxArray = np.array([[1],[0],[0],[0],[0],[0],[0],[0],[0],[0]])\n",
    "desiredOutput = [np.roll(auxArray,x) for x in y]\n",
    "desiredOutput = np.array(desiredOutput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "#in this part we include which features we want to give to ANN and combine them with predictions\n",
    "USING_PREDICTIONS = True\n",
    "data = pd.read_csv('Normalised_Features.csv')\n",
    "features_selection = ['chroma_stft','spectral_centroid','spectral_bandwidth','rolloff','zero_crossing_rate', 'mfcc']\n",
    "new_feature_table = pd.DataFrame()\n",
    "\n",
    "for feature in features_selection:\n",
    "    if feature != 'mfcc':\n",
    "        new_feature_table = pd.concat([new_feature_table, data.loc[:,[feature]]],axis=1)\n",
    "    else:\n",
    "        new_feature_table = pd.concat([new_feature_table, data.filter(regex=(\"mfcc.*\"))],axis=1)\n",
    "\n",
    "if USING_PREDICTIONS:       \n",
    "    features_and_predictions = pd.concat([new_feature_table,data.loc[:,supported_genres]],axis=1)\n",
    "else:\n",
    "    features_and_predictions = new_feature_table   \n",
    "\n",
    "ANN_NUM_OF_INPUT = len(features_and_predictions.columns)\n",
    "ANN_NUM_OF_OUTPUT = len(supported_genres)\n",
    "#features_and_predictions[:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set=[]\n",
    "\n",
    "for row_index,row in features_and_predictions.iterrows():\n",
    "    r = np.array(row)\n",
    "    data_set.append(np.reshape(r,(len(row),1)))\n",
    "data_set = [(x,y) for x,y in zip(data_set,desiredOutput)]\n",
    "#random.shuffle(data_set)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_DATASET_P = 0.5\n",
    "EVALUATION_DATASET_P = 0.2\n",
    "TEST_DATASET_P = 0.3\n",
    "\n",
    "l = len(data_set)\n",
    "frames_per_song = 19 #we have 4 samples for 1 song\n",
    "num_of_songs = int(l/frames_per_song)\n",
    "songs = np.array(data_set).reshape(num_of_songs,frames_per_song,2)\n",
    "#np.random.shuffle(songs)\n",
    "\n",
    "i = int(num_of_songs*LEARNING_DATASET_P)\n",
    "j = int(num_of_songs*EVALUATION_DATASET_P)\n",
    "t = int(num_of_songs*TEST_DATASET_P)\n",
    "\n",
    "test_dataset= np.reshape(songs[:t],(t*frames_per_song,2))\n",
    "learning_dataset = np.reshape(songs[t:t+i],(i*frames_per_song,2))\n",
    "evaluation_dataset = np.reshape(songs[t+i:t+i+j],(j*frames_per_song,2))\n",
    "\n",
    "np.random.shuffle(learning_dataset)\n",
    "np.random.shuffle(evaluation_dataset)\n",
    "np.random.shuffle(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann = Network([ANN_NUM_OF_INPUT,20,15,ANN_NUM_OF_OUTPUT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 3237 / 3800, 0.8518421052631578%\n",
      "0\n",
      "Epoch 1: 3248 / 3800, 0.8547368421052631%\n",
      "0\n",
      "Epoch 2: 3262 / 3800, 0.858421052631579%\n",
      "0\n",
      "Epoch 3: 3269 / 3800, 0.8602631578947368%\n",
      "0\n",
      "Epoch 4: 3274 / 3800, 0.861578947368421%\n",
      "0\n",
      "Epoch 5: 3266 / 3800, 0.8594736842105263%\n",
      "1\n",
      "Epoch 6: 3265 / 3800, 0.8592105263157894%\n",
      "2\n",
      "Epoch 7: 3257 / 3800, 0.8571052631578947%\n",
      "3\n",
      "Epoch 8: 3264 / 3800, 0.8589473684210527%\n",
      "4\n",
      "Epoch 9: 3265 / 3800, 0.8592105263157894%\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "ann.SGD(learning_dataset.tolist(),500,32,0.01,test_data=evaluation_dataset.tolist(), using_soft_max=True,using_adam_optimizer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann.loadWeightsAndBiases(\"Network2.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9763157894736842\n"
     ]
    }
   ],
   "source": [
    "print(ann.evaluate(test_data=test_dataset.tolist(),soft_max=True)/len(evaluation_dataset.tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

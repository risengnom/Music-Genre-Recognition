{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "Currently using code taken from\n",
    "https://gist.github.com/parulnith/7f8c174e6ac099e86f0495d3d9a4c01e#file-music_genre_classification-ipynb\n",
    "for exploration of possible solutions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cNnM2w-HCeb1"
   },
   "source": [
    "# Music genre classification notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2l3sppZMCydR"
   },
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gt3fyg6dCNvX"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# feature extractoring and preprocessing data\n",
    "import librosa\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from PIL import Image\n",
    "import pathlib\n",
    "import csv\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "#Keras\n",
    "import keras\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPe_ebYuDqr5"
   },
   "source": [
    "## Extracting music and features\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We use [GTZAN genre collection](http://marsyasweb.appspot.com/download/data_sets/) dataset for classification. \n",
    "<br>\n",
    "<br>\n",
    "The dataset consists of 10 genres i.e\n",
    " * Blues\n",
    " * Classical\n",
    " * Country\n",
    " * Disco\n",
    " * Hiphop\n",
    " * Jazz\n",
    " * Metal\n",
    " * Pop\n",
    " * Reggae\n",
    " * Rock\n",
    " \n",
    "Each genre contains 100 songs. Total dataset: 1000 songs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "neqMS0VoDpN5"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AfBSVfRCD3PE"
   },
   "source": [
    "## Extracting the Spectrogram for every Audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BHh3pTEVDdrT"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 720x720 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.get_cmap('inferno')\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "for g in genres:\n",
    "    pathlib.Path(f'img_data/{g}').mkdir(parents=True, exist_ok=True)     \n",
    "    for filename in os.listdir(f'../../audio/testfiles/GTZAN/genres/{g}'):\n",
    "        songname = f'../../audio/testfiles/GTZAN/genres/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=5)\n",
    "        plt.specgram(y, NFFT=2048, Fs=2, Fc=0, noverlap=128, cmap=cmap, sides='default', mode='default', scale='dB');\n",
    "        plt.axis('off');\n",
    "        plt.savefig(f'img_data/{g}/{filename[:-3].replace(\".\", \"\")}.png')\n",
    "        plt.clf()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SszVgjYnFNX9"
   },
   "source": [
    "All the audio files get converted into their respective spectrograms .WE can noe easily extract features from them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Nw9HpSdFRsW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "piwUwgP5Eef9"
   },
   "source": [
    "## Extracting features from Spectrogram\n",
    "\n",
    "\n",
    "We will extract\n",
    "\n",
    "* Mel-frequency cepstral coefficients (MFCC)(20 in number)\n",
    "* Spectral Centroid,\n",
    "* Zero Crossing Rate\n",
    "* Chroma Frequencies\n",
    "* Spectral Roll-off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "__g8tX8pDeIL"
   },
   "outputs": [],
   "source": [
    "header = 'filename chroma_stft spectral_centroid spectral_bandwidth rolloff zero_crossing_rate'\n",
    "for i in range(1, 21):\n",
    "    header += f' mfcc{i}'\n",
    "header += ' label'\n",
    "header = header.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TBlT448pEqR9"
   },
   "source": [
    "## Writing data to csv file\n",
    "\n",
    "We write the data to a csv file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZsSQmB0PE3Iu"
   },
   "outputs": [],
   "source": [
    "file = open('data.csv', 'w', newline='')\n",
    "with file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(header)\n",
    "genres = 'blues classical country disco hiphop jazz metal pop reggae rock'.split()\n",
    "filepath = '../../audio/testfiles/GTZAN/genres/'\n",
    "for g in genres:\n",
    "    for filename in os.listdir(f'{filepath}/{g}'):\n",
    "        songname = f'{filepath}/{g}/{filename}'\n",
    "        y, sr = librosa.load(songname, mono=True, duration=30)\n",
    "        chroma_stft = librosa.feature.chroma_stft(y=y, sr=sr)\n",
    "        #rmse = librosa.feature.rmse(y=y, S=None, frame_length=2048, hop_length=512, center=True, pad_mode='reflect')\n",
    "        spec_cent = librosa.feature.spectral_centroid(y=y, sr=sr)\n",
    "        spec_bw = librosa.feature.spectral_bandwidth(y=y, sr=sr)\n",
    "        rolloff = librosa.feature.spectral_rolloff(y=y, sr=sr)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y)\n",
    "        mfcc = librosa.feature.mfcc(y=y, sr=sr)\n",
    "        #to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(rmse)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        to_append = f'{filename} {np.mean(chroma_stft)} {np.mean(spec_cent)} {np.mean(spec_bw)} {np.mean(rolloff)} {np.mean(zcr)}'    \n",
    "        for e in mfcc:\n",
    "            to_append += f' {np.mean(e)}'\n",
    "        to_append += f' {g}'\n",
    "        file = open('data.csv', 'a', newline='')\n",
    "        with file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(to_append.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0yfdo1cj6V7d"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fgeCZSKQEp1A"
   },
   "source": [
    "# Analysing the Data in Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "id": "Kr5_EdpD9dyh",
    "outputId": "81fd4a29-93fa-44f8-bf90-2f99981f761a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>chroma_stft</th>\n",
       "      <th>spectral_centroid</th>\n",
       "      <th>spectral_bandwidth</th>\n",
       "      <th>rolloff</th>\n",
       "      <th>zero_crossing_rate</th>\n",
       "      <th>mfcc1</th>\n",
       "      <th>mfcc2</th>\n",
       "      <th>mfcc3</th>\n",
       "      <th>mfcc4</th>\n",
       "      <th>...</th>\n",
       "      <th>mfcc12</th>\n",
       "      <th>mfcc13</th>\n",
       "      <th>mfcc14</th>\n",
       "      <th>mfcc15</th>\n",
       "      <th>mfcc16</th>\n",
       "      <th>mfcc17</th>\n",
       "      <th>mfcc18</th>\n",
       "      <th>mfcc19</th>\n",
       "      <th>mfcc20</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>blues.00085.wav</td>\n",
       "      <td>0.315363</td>\n",
       "      <td>1312.308199</td>\n",
       "      <td>1673.915613</td>\n",
       "      <td>2638.117381</td>\n",
       "      <td>0.059416</td>\n",
       "      <td>-180.123596</td>\n",
       "      <td>131.420259</td>\n",
       "      <td>0.566188</td>\n",
       "      <td>43.152929</td>\n",
       "      <td>...</td>\n",
       "      <td>2.718054</td>\n",
       "      <td>0.772589</td>\n",
       "      <td>1.997588</td>\n",
       "      <td>-6.093858</td>\n",
       "      <td>3.484042</td>\n",
       "      <td>-8.341479</td>\n",
       "      <td>3.204648</td>\n",
       "      <td>-0.926944</td>\n",
       "      <td>-2.243686</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>blues.00051.wav</td>\n",
       "      <td>0.393756</td>\n",
       "      <td>1977.172377</td>\n",
       "      <td>1927.803692</td>\n",
       "      <td>3942.834492</td>\n",
       "      <td>0.106627</td>\n",
       "      <td>-55.579243</td>\n",
       "      <td>114.935848</td>\n",
       "      <td>-37.052831</td>\n",
       "      <td>64.896513</td>\n",
       "      <td>...</td>\n",
       "      <td>12.782317</td>\n",
       "      <td>-16.528681</td>\n",
       "      <td>3.793788</td>\n",
       "      <td>-7.890870</td>\n",
       "      <td>8.477609</td>\n",
       "      <td>-4.065210</td>\n",
       "      <td>3.207441</td>\n",
       "      <td>-5.178250</td>\n",
       "      <td>-1.279524</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>blues.00038.wav</td>\n",
       "      <td>0.265883</td>\n",
       "      <td>1513.422107</td>\n",
       "      <td>2140.606779</td>\n",
       "      <td>3449.679140</td>\n",
       "      <td>0.044378</td>\n",
       "      <td>-192.667187</td>\n",
       "      <td>111.190774</td>\n",
       "      <td>21.372723</td>\n",
       "      <td>26.398341</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.044727</td>\n",
       "      <td>-17.631820</td>\n",
       "      <td>-7.921687</td>\n",
       "      <td>-15.152404</td>\n",
       "      <td>-12.342546</td>\n",
       "      <td>-17.227765</td>\n",
       "      <td>-7.771728</td>\n",
       "      <td>-13.033961</td>\n",
       "      <td>-17.945750</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>blues.00069.wav</td>\n",
       "      <td>0.291884</td>\n",
       "      <td>2371.099278</td>\n",
       "      <td>2209.699346</td>\n",
       "      <td>5004.111407</td>\n",
       "      <td>0.125141</td>\n",
       "      <td>-121.408845</td>\n",
       "      <td>96.106080</td>\n",
       "      <td>-19.613255</td>\n",
       "      <td>48.011654</td>\n",
       "      <td>...</td>\n",
       "      <td>6.793616</td>\n",
       "      <td>-19.333882</td>\n",
       "      <td>-0.114447</td>\n",
       "      <td>-12.250433</td>\n",
       "      <td>-4.927438</td>\n",
       "      <td>-10.409581</td>\n",
       "      <td>-2.049566</td>\n",
       "      <td>-1.768211</td>\n",
       "      <td>-3.962159</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>blues.00049.wav</td>\n",
       "      <td>0.277484</td>\n",
       "      <td>1318.656822</td>\n",
       "      <td>1904.761177</td>\n",
       "      <td>3046.681577</td>\n",
       "      <td>0.039973</td>\n",
       "      <td>-255.965774</td>\n",
       "      <td>110.823046</td>\n",
       "      <td>21.505215</td>\n",
       "      <td>67.571233</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.442929</td>\n",
       "      <td>-7.130865</td>\n",
       "      <td>-11.163917</td>\n",
       "      <td>-12.609408</td>\n",
       "      <td>2.742009</td>\n",
       "      <td>-8.948849</td>\n",
       "      <td>-7.940728</td>\n",
       "      <td>-2.344157</td>\n",
       "      <td>-0.705606</td>\n",
       "      <td>blues</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          filename  chroma_stft  spectral_centroid  spectral_bandwidth  \\\n",
       "0  blues.00085.wav     0.315363        1312.308199         1673.915613   \n",
       "1  blues.00051.wav     0.393756        1977.172377         1927.803692   \n",
       "2  blues.00038.wav     0.265883        1513.422107         2140.606779   \n",
       "3  blues.00069.wav     0.291884        2371.099278         2209.699346   \n",
       "4  blues.00049.wav     0.277484        1318.656822         1904.761177   \n",
       "\n",
       "       rolloff  zero_crossing_rate       mfcc1       mfcc2      mfcc3  \\\n",
       "0  2638.117381            0.059416 -180.123596  131.420259   0.566188   \n",
       "1  3942.834492            0.106627  -55.579243  114.935848 -37.052831   \n",
       "2  3449.679140            0.044378 -192.667187  111.190774  21.372723   \n",
       "3  5004.111407            0.125141 -121.408845   96.106080 -19.613255   \n",
       "4  3046.681577            0.039973 -255.965774  110.823046  21.505215   \n",
       "\n",
       "       mfcc4  ...     mfcc12     mfcc13     mfcc14     mfcc15     mfcc16  \\\n",
       "0  43.152929  ...   2.718054   0.772589   1.997588  -6.093858   3.484042   \n",
       "1  64.896513  ...  12.782317 -16.528681   3.793788  -7.890870   8.477609   \n",
       "2  26.398341  ...  -5.044727 -17.631820  -7.921687 -15.152404 -12.342546   \n",
       "3  48.011654  ...   6.793616 -19.333882  -0.114447 -12.250433  -4.927438   \n",
       "4  67.571233  ...  -2.442929  -7.130865 -11.163917 -12.609408   2.742009   \n",
       "\n",
       "      mfcc17    mfcc18     mfcc19     mfcc20  label  \n",
       "0  -8.341479  3.204648  -0.926944  -2.243686  blues  \n",
       "1  -4.065210  3.207441  -5.178250  -1.279524  blues  \n",
       "2 -17.227765 -7.771728 -13.033961 -17.945750  blues  \n",
       "3 -10.409581 -2.049566  -1.768211  -3.962159  blues  \n",
       "4  -8.948849 -7.940728  -2.344157  -0.705606  blues  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('data.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iHrDHCaR9gKR",
    "outputId": "7d32943a-1ad5-4a59-c13a-beebeb36e4c2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 27)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "veD5BgX49hZa"
   },
   "outputs": [],
   "source": [
    "# Dropping unneccesary columns\n",
    "data = data.drop(['filename'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Nyr0aAAsGXjZ"
   },
   "source": [
    "## Encoding the Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "frI5HH4q-1HS"
   },
   "outputs": [],
   "source": [
    "genre_list = data.iloc[:, -1]\n",
    "encoder = LabelEncoder()\n",
    "y = encoder.fit_transform(genre_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Slm8W0-iGVhI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_2n8a02zGfvP"
   },
   "source": [
    "## Scaling the Feature columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uqcqn-nyAofk"
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(np.array(data.iloc[:, :-1], dtype = float))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e3VZvbwpGo9R"
   },
   "source": [
    "## Dividing data into training and Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1GW3VvQA7Rj"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "upuczQ-KBHJ5",
    "outputId": "1431a28b-e8b6-4db2-e505-7e149e37c0d7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "800"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LtoE_FqqBzM8",
    "outputId": "76555a2b-2030-48e1-b52d-d71b4ebae38e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "ir9XaWgQB0lq",
    "outputId": "2ec90814-19d8-4f27-934a-1ce54406d4ea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.0574891 ,  2.58660906,  2.24995798,  2.42092734,  2.56082714,\n",
       "        0.91680989, -1.65333673,  1.03200834, -2.04530466,  1.17908317,\n",
       "       -0.4646132 ,  1.96990119, -0.88415511,  1.98758265, -0.09116011,\n",
       "        0.78221431, -0.40849949,  2.15487121,  1.02655532,  1.44677634,\n",
       "        0.24468331,  0.67515747,  0.39590775,  1.22952235,  1.19075062])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vp2yc5FWG04e"
   },
   "source": [
    "# Classification with Keras\n",
    "\n",
    "## Building our Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qj3sc2uFEUMt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/carsten/.local/share/virtualenvs/notebook-am4AQewu/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "\n",
    "model.add(layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yrsmpI6EjJ2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "bP0hVm4aElS7",
    "outputId": "aacf234d-d0a9-4de4-91be-5fd45a33b279"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/carsten/.local/share/virtualenvs/notebook-am4AQewu/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/200\n",
      "800/800 [==============================] - 0s 418us/step - loss: 2.2025 - acc: 0.2025\n",
      "Epoch 2/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 1.9097 - acc: 0.3588\n",
      "Epoch 3/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 1.6857 - acc: 0.4025\n",
      "Epoch 4/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 1.4918 - acc: 0.4863\n",
      "Epoch 5/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 1.3394 - acc: 0.5325\n",
      "Epoch 6/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 1.2260 - acc: 0.5862\n",
      "Epoch 7/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 1.1501 - acc: 0.6062\n",
      "Epoch 8/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 1.0687 - acc: 0.6225\n",
      "Epoch 9/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.9914 - acc: 0.6713\n",
      "Epoch 10/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.9397 - acc: 0.6900\n",
      "Epoch 11/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.8787 - acc: 0.7050\n",
      "Epoch 12/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.8321 - acc: 0.7188\n",
      "Epoch 13/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.7876 - acc: 0.7462\n",
      "Epoch 14/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.7451 - acc: 0.7638\n",
      "Epoch 15/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.7084 - acc: 0.7812\n",
      "Epoch 16/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.6794 - acc: 0.7838\n",
      "Epoch 17/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.6519 - acc: 0.7925\n",
      "Epoch 18/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.6168 - acc: 0.8038\n",
      "Epoch 19/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.5882 - acc: 0.8175\n",
      "Epoch 20/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.5555 - acc: 0.8362\n",
      "Epoch 21/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.5234 - acc: 0.8513\n",
      "Epoch 22/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.5072 - acc: 0.8412\n",
      "Epoch 23/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.4857 - acc: 0.8600\n",
      "Epoch 24/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.4711 - acc: 0.8625\n",
      "Epoch 25/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.4374 - acc: 0.8750\n",
      "Epoch 26/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.4090 - acc: 0.8800\n",
      "Epoch 27/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.4020 - acc: 0.8775\n",
      "Epoch 28/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.3868 - acc: 0.8850\n",
      "Epoch 29/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.3678 - acc: 0.8962\n",
      "Epoch 30/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.3433 - acc: 0.9025\n",
      "Epoch 31/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.3298 - acc: 0.9113\n",
      "Epoch 32/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.3151 - acc: 0.9175\n",
      "Epoch 33/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.3017 - acc: 0.9200\n",
      "Epoch 34/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.2713 - acc: 0.9375\n",
      "Epoch 35/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.2599 - acc: 0.9375\n",
      "Epoch 36/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.2486 - acc: 0.9363\n",
      "Epoch 37/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.2473 - acc: 0.9425\n",
      "Epoch 38/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.2327 - acc: 0.9462\n",
      "Epoch 39/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.2194 - acc: 0.9612\n",
      "Epoch 40/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.2062 - acc: 0.9612\n",
      "Epoch 41/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.1982 - acc: 0.9563\n",
      "Epoch 42/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.1872 - acc: 0.9600\n",
      "Epoch 43/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.1743 - acc: 0.9750\n",
      "Epoch 44/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.1727 - acc: 0.9650\n",
      "Epoch 45/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.1657 - acc: 0.9725\n",
      "Epoch 46/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.1654 - acc: 0.9612\n",
      "Epoch 47/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1592 - acc: 0.9688\n",
      "Epoch 48/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.1523 - acc: 0.9662\n",
      "Epoch 49/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1366 - acc: 0.9775\n",
      "Epoch 50/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.1318 - acc: 0.9738\n",
      "Epoch 51/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.1183 - acc: 0.9875\n",
      "Epoch 52/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.1076 - acc: 0.9937\n",
      "Epoch 53/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.1017 - acc: 0.9913\n",
      "Epoch 54/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.1013 - acc: 0.9862\n",
      "Epoch 55/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.1084 - acc: 0.9862\n",
      "Epoch 56/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0977 - acc: 0.9875\n",
      "Epoch 57/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0905 - acc: 0.9925\n",
      "Epoch 58/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0837 - acc: 0.9925\n",
      "Epoch 59/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0799 - acc: 0.9950\n",
      "Epoch 60/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0773 - acc: 0.9925\n",
      "Epoch 61/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0749 - acc: 0.9950\n",
      "Epoch 62/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.0686 - acc: 0.9937\n",
      "Epoch 63/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0633 - acc: 0.9950\n",
      "Epoch 64/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0672 - acc: 0.9950\n",
      "Epoch 65/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0609 - acc: 0.9962\n",
      "Epoch 66/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0555 - acc: 0.9962\n",
      "Epoch 67/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0586 - acc: 0.9950\n",
      "Epoch 68/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0516 - acc: 0.9975\n",
      "Epoch 69/200\n",
      "800/800 [==============================] - 0s 44us/step - loss: 0.0507 - acc: 0.9975\n",
      "Epoch 70/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0450 - acc: 0.9962\n",
      "Epoch 71/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0425 - acc: 0.9975\n",
      "Epoch 72/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0403 - acc: 0.9975\n",
      "Epoch 73/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0477 - acc: 0.9962\n",
      "Epoch 74/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0403 - acc: 0.9975\n",
      "Epoch 75/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0367 - acc: 0.9975\n",
      "Epoch 76/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0370 - acc: 0.9962\n",
      "Epoch 77/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0355 - acc: 0.9975\n",
      "Epoch 78/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0341 - acc: 0.9975\n",
      "Epoch 79/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0354 - acc: 0.9975\n",
      "Epoch 80/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0313 - acc: 0.9975\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 31us/step - loss: 0.0403 - acc: 0.9988\n",
      "Epoch 82/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0332 - acc: 0.9975\n",
      "Epoch 83/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0323 - acc: 0.9975\n",
      "Epoch 84/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0350 - acc: 0.9962\n",
      "Epoch 85/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0312 - acc: 0.9962\n",
      "Epoch 86/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0254 - acc: 0.9975\n",
      "Epoch 87/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0235 - acc: 0.9975\n",
      "Epoch 88/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0236 - acc: 0.9975\n",
      "Epoch 89/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0224 - acc: 0.9988\n",
      "Epoch 90/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0219 - acc: 0.9975\n",
      "Epoch 91/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0224 - acc: 0.9988\n",
      "Epoch 92/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0200 - acc: 0.9962\n",
      "Epoch 93/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0196 - acc: 0.9975\n",
      "Epoch 94/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0200 - acc: 0.9988\n",
      "Epoch 95/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0181 - acc: 0.9975\n",
      "Epoch 96/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.0179 - acc: 0.9988\n",
      "Epoch 97/200\n",
      "800/800 [==============================] - 0s 39us/step - loss: 0.0189 - acc: 0.9988\n",
      "Epoch 98/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0165 - acc: 0.9988\n",
      "Epoch 99/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0158 - acc: 0.9988\n",
      "Epoch 100/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0163 - acc: 0.9988\n",
      "Epoch 101/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0150 - acc: 0.9988\n",
      "Epoch 102/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0150 - acc: 0.9988\n",
      "Epoch 103/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0156 - acc: 0.9988\n",
      "Epoch 104/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0176 - acc: 0.9975\n",
      "Epoch 105/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0381 - acc: 0.9937\n",
      "Epoch 106/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0462 - acc: 0.9875\n",
      "Epoch 107/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0460 - acc: 0.9900\n",
      "Epoch 108/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0280 - acc: 0.9962\n",
      "Epoch 109/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0180 - acc: 0.9988\n",
      "Epoch 110/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0207 - acc: 0.9988\n",
      "Epoch 111/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0364 - acc: 0.9950\n",
      "Epoch 112/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0234 - acc: 0.9975\n",
      "Epoch 113/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0270 - acc: 0.9962\n",
      "Epoch 114/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0228 - acc: 0.9988\n",
      "Epoch 115/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0239 - acc: 0.9925\n",
      "Epoch 116/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0179 - acc: 0.9988\n",
      "Epoch 117/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0175 - acc: 0.9988\n",
      "Epoch 118/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0120 - acc: 0.9988\n",
      "Epoch 119/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0119 - acc: 0.9988\n",
      "Epoch 120/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.0104 - acc: 0.9988\n",
      "Epoch 121/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0109 - acc: 0.9975\n",
      "Epoch 122/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0096 - acc: 0.9988\n",
      "Epoch 123/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0095 - acc: 0.9988\n",
      "Epoch 124/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0096 - acc: 0.9988\n",
      "Epoch 125/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0093 - acc: 0.9988\n",
      "Epoch 126/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0089 - acc: 0.9988\n",
      "Epoch 127/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0091 - acc: 0.9988\n",
      "Epoch 128/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0088 - acc: 0.9988\n",
      "Epoch 129/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0090 - acc: 0.9975\n",
      "Epoch 130/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0080 - acc: 0.9988\n",
      "Epoch 131/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0085 - acc: 0.9975\n",
      "Epoch 132/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0094 - acc: 0.9988\n",
      "Epoch 133/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0082 - acc: 0.9988\n",
      "Epoch 134/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0093 - acc: 0.9988\n",
      "Epoch 135/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0096 - acc: 0.9988\n",
      "Epoch 136/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0083 - acc: 0.9988\n",
      "Epoch 137/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0097 - acc: 0.9975\n",
      "Epoch 138/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0095 - acc: 0.9975\n",
      "Epoch 139/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0081 - acc: 0.9988\n",
      "Epoch 140/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0075 - acc: 0.9975\n",
      "Epoch 141/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0074 - acc: 0.9988\n",
      "Epoch 142/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0068 - acc: 0.9988\n",
      "Epoch 143/200\n",
      "800/800 [==============================] - 0s 34us/step - loss: 0.0068 - acc: 0.9975\n",
      "Epoch 144/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0071 - acc: 0.9988\n",
      "Epoch 145/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0087 - acc: 0.9975\n",
      "Epoch 146/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0064 - acc: 0.9988\n",
      "Epoch 147/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0079 - acc: 0.9988\n",
      "Epoch 148/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0082 - acc: 0.9975\n",
      "Epoch 149/200\n",
      "800/800 [==============================] - 0s 36us/step - loss: 0.0063 - acc: 0.9988\n",
      "Epoch 150/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0064 - acc: 0.9988\n",
      "Epoch 151/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0067 - acc: 0.9988\n",
      "Epoch 152/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0065 - acc: 0.9988\n",
      "Epoch 153/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0069 - acc: 0.9975\n",
      "Epoch 154/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0122 - acc: 0.9988\n",
      "Epoch 155/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0107 - acc: 0.9988\n",
      "Epoch 156/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0085 - acc: 0.9988\n",
      "Epoch 157/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0286 - acc: 0.9937\n",
      "Epoch 158/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.0336 - acc: 0.9900\n",
      "Epoch 159/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0182 - acc: 0.9937\n",
      "Epoch 160/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0150 - acc: 0.9962\n",
      "Epoch 161/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0092 - acc: 0.9975\n",
      "Epoch 162/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0092 - acc: 0.9988\n",
      "Epoch 163/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0069 - acc: 0.9988\n",
      "Epoch 164/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "800/800 [==============================] - 0s 29us/step - loss: 0.0065 - acc: 0.9988\n",
      "Epoch 165/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0068 - acc: 0.9988\n",
      "Epoch 166/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0193 - acc: 0.9975\n",
      "Epoch 167/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0118 - acc: 0.9950\n",
      "Epoch 168/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0100 - acc: 0.9975\n",
      "Epoch 169/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0077 - acc: 0.9988\n",
      "Epoch 170/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0060 - acc: 0.9988\n",
      "Epoch 171/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0055 - acc: 0.9988\n",
      "Epoch 172/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0063 - acc: 0.9988\n",
      "Epoch 173/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0053 - acc: 0.9975\n",
      "Epoch 174/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0057 - acc: 0.9975\n",
      "Epoch 175/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0070 - acc: 0.9988\n",
      "Epoch 176/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0093 - acc: 0.9988\n",
      "Epoch 177/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0084 - acc: 0.9975\n",
      "Epoch 178/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 179/200\n",
      "800/800 [==============================] - 0s 33us/step - loss: 0.0129 - acc: 0.9988\n",
      "Epoch 180/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0081 - acc: 0.9988\n",
      "Epoch 181/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0056 - acc: 0.9988\n",
      "Epoch 182/200\n",
      "800/800 [==============================] - 0s 35us/step - loss: 0.0054 - acc: 0.9988\n",
      "Epoch 183/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 184/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0052 - acc: 0.9988\n",
      "Epoch 185/200\n",
      "800/800 [==============================] - 0s 37us/step - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 186/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0058 - acc: 0.9988\n",
      "Epoch 187/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0105 - acc: 0.9988\n",
      "Epoch 188/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0059 - acc: 0.9988\n",
      "Epoch 189/200\n",
      "800/800 [==============================] - 0s 27us/step - loss: 0.0075 - acc: 0.9988\n",
      "Epoch 190/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0044 - acc: 0.9988\n",
      "Epoch 191/200\n",
      "800/800 [==============================] - 0s 38us/step - loss: 0.0045 - acc: 0.9988\n",
      "Epoch 192/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0048 - acc: 0.9988\n",
      "Epoch 193/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0043 - acc: 0.9988\n",
      "Epoch 194/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0046 - acc: 0.9988\n",
      "Epoch 195/200\n",
      "800/800 [==============================] - 0s 31us/step - loss: 0.0055 - acc: 0.9975\n",
      "Epoch 196/200\n",
      "800/800 [==============================] - 0s 29us/step - loss: 0.0047 - acc: 0.9988\n",
      "Epoch 197/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0056 - acc: 0.9988\n",
      "Epoch 198/200\n",
      "800/800 [==============================] - 0s 30us/step - loss: 0.0128 - acc: 0.9975\n",
      "Epoch 199/200\n",
      "800/800 [==============================] - 0s 32us/step - loss: 0.0090 - acc: 0.9975\n",
      "Epoch 200/200\n",
      "800/800 [==============================] - 0s 28us/step - loss: 0.0067 - acc: 0.9988\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    epochs=200,\n",
    "                    batch_size=128)\n",
    "                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0m1J0_wUFK4C",
    "outputId": "ffd3bf36-29ea-437a-987c-9aa600b9dae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200/200 [==============================] - 0s 195us/step\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f6HrjXeUF0Ko",
    "outputId": "ea282dbd-6f9e-48c7-de2d-dc9afde8949e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_acc:  0.635\n"
     ]
    }
   ],
   "source": [
    "print('test_acc: ',test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3yQmP_f5Kq0w"
   },
   "source": [
    "Tes accuracy is less than training dataa accuracy. This hints at Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-U2qzRJoHV9O"
   },
   "source": [
    "## Validating our approach\n",
    "Let's set apart 200 samples in our training data to use as a validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xJNbvYZoF7ZT"
   },
   "outputs": [],
   "source": [
    "x_val = X_train[:200]\n",
    "partial_x_train = X_train[200:]\n",
    "\n",
    "y_val = y_train[:200]\n",
    "partial_y_train = y_train[200:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L1EkG59EHeEV"
   },
   "source": [
    "Now let's train our network for 20 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1071
    },
    "colab_type": "code",
    "id": "Dp3G4P3aP4k2",
    "outputId": "25e1a389-1ac2-425b-bd5f-05736b6e9b96",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 600 samples, validate on 200 samples\n",
      "Epoch 1/200\n",
      "600/600 [==============================] - 0s 593us/step - loss: 2.2892 - acc: 0.1000 - val_loss: 2.1609 - val_acc: 0.2950\n",
      "Epoch 2/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 2.1185 - acc: 0.3350 - val_loss: 2.0363 - val_acc: 0.3200\n",
      "Epoch 3/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 1.9659 - acc: 0.3733 - val_loss: 1.9150 - val_acc: 0.3500\n",
      "Epoch 4/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.8125 - acc: 0.3833 - val_loss: 1.7963 - val_acc: 0.3750\n",
      "Epoch 5/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 1.6777 - acc: 0.3967 - val_loss: 1.6781 - val_acc: 0.4350\n",
      "Epoch 6/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 1.5606 - acc: 0.4483 - val_loss: 1.5703 - val_acc: 0.4550\n",
      "Epoch 7/200\n",
      "600/600 [==============================] - 0s 41us/step - loss: 1.4577 - acc: 0.4867 - val_loss: 1.4844 - val_acc: 0.4650\n",
      "Epoch 8/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 1.3648 - acc: 0.5200 - val_loss: 1.4393 - val_acc: 0.4700\n",
      "Epoch 9/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 1.2888 - acc: 0.5483 - val_loss: 1.3832 - val_acc: 0.4800\n",
      "Epoch 10/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 1.2207 - acc: 0.5900 - val_loss: 1.3083 - val_acc: 0.5400\n",
      "Epoch 11/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 1.1688 - acc: 0.6083 - val_loss: 1.2254 - val_acc: 0.5700\n",
      "Epoch 12/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 1.0944 - acc: 0.6250 - val_loss: 1.2004 - val_acc: 0.5500\n",
      "Epoch 13/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 1.0481 - acc: 0.6333 - val_loss: 1.1617 - val_acc: 0.5750\n",
      "Epoch 14/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.9971 - acc: 0.6700 - val_loss: 1.1459 - val_acc: 0.5850\n",
      "Epoch 15/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.9640 - acc: 0.6750 - val_loss: 1.1328 - val_acc: 0.5850\n",
      "Epoch 16/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.9206 - acc: 0.6817 - val_loss: 1.1129 - val_acc: 0.5900\n",
      "Epoch 17/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.8923 - acc: 0.6933 - val_loss: 1.0476 - val_acc: 0.6500\n",
      "Epoch 18/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.8527 - acc: 0.7117 - val_loss: 1.0587 - val_acc: 0.6300\n",
      "Epoch 19/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.8430 - acc: 0.7033 - val_loss: 1.0800 - val_acc: 0.6400\n",
      "Epoch 20/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.8137 - acc: 0.7117 - val_loss: 1.0480 - val_acc: 0.6400\n",
      "Epoch 21/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.7784 - acc: 0.7433 - val_loss: 1.0078 - val_acc: 0.6550\n",
      "Epoch 22/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.7535 - acc: 0.7467 - val_loss: 0.9878 - val_acc: 0.6450\n",
      "Epoch 23/200\n",
      "600/600 [==============================] - 0s 52us/step - loss: 0.7208 - acc: 0.7700 - val_loss: 0.9726 - val_acc: 0.6550\n",
      "Epoch 24/200\n",
      "600/600 [==============================] - 0s 54us/step - loss: 0.6871 - acc: 0.7717 - val_loss: 0.9792 - val_acc: 0.6750\n",
      "Epoch 25/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.6605 - acc: 0.7867 - val_loss: 0.9582 - val_acc: 0.6800\n",
      "Epoch 26/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.6354 - acc: 0.7867 - val_loss: 0.9385 - val_acc: 0.6900\n",
      "Epoch 27/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.6172 - acc: 0.7817 - val_loss: 0.9301 - val_acc: 0.6700\n",
      "Epoch 28/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.5901 - acc: 0.8083 - val_loss: 0.9401 - val_acc: 0.6650\n",
      "Epoch 29/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.5685 - acc: 0.8183 - val_loss: 0.9520 - val_acc: 0.6600\n",
      "Epoch 30/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.5492 - acc: 0.8167 - val_loss: 0.9634 - val_acc: 0.6650\n",
      "Epoch 31/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.5288 - acc: 0.8233 - val_loss: 0.9472 - val_acc: 0.6650\n",
      "Epoch 32/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.5038 - acc: 0.8367 - val_loss: 0.9283 - val_acc: 0.6750\n",
      "Epoch 33/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.4995 - acc: 0.8550 - val_loss: 0.8993 - val_acc: 0.6900\n",
      "Epoch 34/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.4746 - acc: 0.8600 - val_loss: 0.9281 - val_acc: 0.6900\n",
      "Epoch 35/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.4587 - acc: 0.8533 - val_loss: 0.9003 - val_acc: 0.6950\n",
      "Epoch 36/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.4156 - acc: 0.8750 - val_loss: 0.9130 - val_acc: 0.6850\n",
      "Epoch 37/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.4162 - acc: 0.8750 - val_loss: 0.8943 - val_acc: 0.6850\n",
      "Epoch 38/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.3942 - acc: 0.8900 - val_loss: 0.9077 - val_acc: 0.6900\n",
      "Epoch 39/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.3784 - acc: 0.8900 - val_loss: 0.9340 - val_acc: 0.6800\n",
      "Epoch 40/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.3633 - acc: 0.8950 - val_loss: 0.9723 - val_acc: 0.6550\n",
      "Epoch 41/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.3656 - acc: 0.8883 - val_loss: 0.9356 - val_acc: 0.6700\n",
      "Epoch 42/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.3349 - acc: 0.9067 - val_loss: 0.9234 - val_acc: 0.6750\n",
      "Epoch 43/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.3293 - acc: 0.9100 - val_loss: 0.9079 - val_acc: 0.6850\n",
      "Epoch 44/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.3163 - acc: 0.9100 - val_loss: 0.9247 - val_acc: 0.6900\n",
      "Epoch 45/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.3139 - acc: 0.9183 - val_loss: 0.9247 - val_acc: 0.7000\n",
      "Epoch 46/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.2913 - acc: 0.9267 - val_loss: 0.9482 - val_acc: 0.6500\n",
      "Epoch 47/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.2834 - acc: 0.9250 - val_loss: 0.9329 - val_acc: 0.6650\n",
      "Epoch 48/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.2562 - acc: 0.9467 - val_loss: 0.9483 - val_acc: 0.6750\n",
      "Epoch 49/200\n",
      "600/600 [==============================] - 0s 51us/step - loss: 0.2664 - acc: 0.9267 - val_loss: 0.9284 - val_acc: 0.6650\n",
      "Epoch 50/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.2450 - acc: 0.9467 - val_loss: 0.9230 - val_acc: 0.6650\n",
      "Epoch 51/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.2346 - acc: 0.9467 - val_loss: 0.9391 - val_acc: 0.6700\n",
      "Epoch 52/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.2167 - acc: 0.9550 - val_loss: 0.9358 - val_acc: 0.6850\n",
      "Epoch 53/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.2087 - acc: 0.9567 - val_loss: 0.9527 - val_acc: 0.6950\n",
      "Epoch 54/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.2181 - acc: 0.9467 - val_loss: 0.9268 - val_acc: 0.6750\n",
      "Epoch 55/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.1977 - acc: 0.9567 - val_loss: 0.9432 - val_acc: 0.6800\n",
      "Epoch 56/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.1894 - acc: 0.9583 - val_loss: 0.9523 - val_acc: 0.6650\n",
      "Epoch 57/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.1766 - acc: 0.9683 - val_loss: 0.9461 - val_acc: 0.6750\n",
      "Epoch 58/200\n",
      "600/600 [==============================] - 0s 55us/step - loss: 0.1711 - acc: 0.9683 - val_loss: 0.9405 - val_acc: 0.6800\n",
      "Epoch 59/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1624 - acc: 0.9750 - val_loss: 0.9722 - val_acc: 0.6700\n",
      "Epoch 60/200\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.1557 - acc: 0.9750 - val_loss: 0.9925 - val_acc: 0.6700\n",
      "Epoch 61/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 44us/step - loss: 0.1463 - acc: 0.9783 - val_loss: 1.0022 - val_acc: 0.6850\n",
      "Epoch 62/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1454 - acc: 0.9767 - val_loss: 1.0014 - val_acc: 0.6900\n",
      "Epoch 63/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1398 - acc: 0.9783 - val_loss: 1.0032 - val_acc: 0.6600\n",
      "Epoch 64/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1256 - acc: 0.9800 - val_loss: 1.0300 - val_acc: 0.6800\n",
      "Epoch 65/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1328 - acc: 0.9733 - val_loss: 0.9986 - val_acc: 0.6800\n",
      "Epoch 66/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.1168 - acc: 0.9800 - val_loss: 0.9939 - val_acc: 0.7000\n",
      "Epoch 67/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.1157 - acc: 0.9817 - val_loss: 1.0161 - val_acc: 0.6850\n",
      "Epoch 68/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1064 - acc: 0.9833 - val_loss: 1.0703 - val_acc: 0.6600\n",
      "Epoch 69/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.1108 - acc: 0.9800 - val_loss: 1.0666 - val_acc: 0.6550\n",
      "Epoch 70/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1007 - acc: 0.9817 - val_loss: 1.0399 - val_acc: 0.6850\n",
      "Epoch 71/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1010 - acc: 0.9833 - val_loss: 1.0553 - val_acc: 0.6750\n",
      "Epoch 72/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.1038 - acc: 0.9850 - val_loss: 1.0501 - val_acc: 0.6700\n",
      "Epoch 73/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0894 - acc: 0.9867 - val_loss: 1.0962 - val_acc: 0.6650\n",
      "Epoch 74/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0929 - acc: 0.9883 - val_loss: 1.1063 - val_acc: 0.7050\n",
      "Epoch 75/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0876 - acc: 0.9850 - val_loss: 1.1361 - val_acc: 0.6700\n",
      "Epoch 76/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.1029 - acc: 0.9783 - val_loss: 1.0484 - val_acc: 0.6700\n",
      "Epoch 77/200\n",
      "600/600 [==============================] - 0s 79us/step - loss: 0.0730 - acc: 0.9883 - val_loss: 1.1040 - val_acc: 0.6650\n",
      "Epoch 78/200\n",
      "600/600 [==============================] - 0s 55us/step - loss: 0.1000 - acc: 0.9867 - val_loss: 1.1032 - val_acc: 0.7000\n",
      "Epoch 79/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0708 - acc: 0.9917 - val_loss: 1.2307 - val_acc: 0.6600\n",
      "Epoch 80/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.1065 - acc: 0.9783 - val_loss: 1.1384 - val_acc: 0.6800\n",
      "Epoch 81/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0716 - acc: 0.9900 - val_loss: 1.1082 - val_acc: 0.6800\n",
      "Epoch 82/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0801 - acc: 0.9867 - val_loss: 1.0840 - val_acc: 0.6850\n",
      "Epoch 83/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0768 - acc: 0.9867 - val_loss: 1.1288 - val_acc: 0.6850\n",
      "Epoch 84/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0687 - acc: 0.9933 - val_loss: 1.1941 - val_acc: 0.6700\n",
      "Epoch 85/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0684 - acc: 0.9950 - val_loss: 1.1701 - val_acc: 0.6900\n",
      "Epoch 86/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0644 - acc: 0.9933 - val_loss: 1.1608 - val_acc: 0.7100\n",
      "Epoch 87/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0616 - acc: 0.9900 - val_loss: 1.1371 - val_acc: 0.7000\n",
      "Epoch 88/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0558 - acc: 0.9933 - val_loss: 1.1456 - val_acc: 0.6800\n",
      "Epoch 89/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0515 - acc: 0.9950 - val_loss: 1.1589 - val_acc: 0.6700\n",
      "Epoch 90/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0505 - acc: 0.9950 - val_loss: 1.1829 - val_acc: 0.6700\n",
      "Epoch 91/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0500 - acc: 0.9950 - val_loss: 1.2145 - val_acc: 0.6700\n",
      "Epoch 92/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0466 - acc: 0.9967 - val_loss: 1.2235 - val_acc: 0.6650\n",
      "Epoch 93/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0425 - acc: 0.9967 - val_loss: 1.2234 - val_acc: 0.6700\n",
      "Epoch 94/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0448 - acc: 0.9950 - val_loss: 1.2066 - val_acc: 0.6700\n",
      "Epoch 95/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0434 - acc: 0.9950 - val_loss: 1.1958 - val_acc: 0.6650\n",
      "Epoch 96/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0392 - acc: 0.9967 - val_loss: 1.2155 - val_acc: 0.6950\n",
      "Epoch 97/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0363 - acc: 0.9933 - val_loss: 1.2567 - val_acc: 0.6900\n",
      "Epoch 98/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0345 - acc: 0.9967 - val_loss: 1.2739 - val_acc: 0.6900\n",
      "Epoch 99/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0367 - acc: 0.9967 - val_loss: 1.2500 - val_acc: 0.7100\n",
      "Epoch 100/200\n",
      "600/600 [==============================] - 0s 51us/step - loss: 0.0335 - acc: 0.9967 - val_loss: 1.2180 - val_acc: 0.6900\n",
      "Epoch 101/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0292 - acc: 0.9967 - val_loss: 1.2244 - val_acc: 0.6800\n",
      "Epoch 102/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0284 - acc: 0.9967 - val_loss: 1.2501 - val_acc: 0.6700\n",
      "Epoch 103/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0284 - acc: 0.9983 - val_loss: 1.2591 - val_acc: 0.6750\n",
      "Epoch 104/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0260 - acc: 0.9983 - val_loss: 1.2587 - val_acc: 0.6800\n",
      "Epoch 105/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0266 - acc: 0.9967 - val_loss: 1.2707 - val_acc: 0.6900\n",
      "Epoch 106/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0244 - acc: 0.9967 - val_loss: 1.2850 - val_acc: 0.6850\n",
      "Epoch 107/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0303 - acc: 0.9950 - val_loss: 1.2857 - val_acc: 0.6600\n",
      "Epoch 108/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0229 - acc: 0.9983 - val_loss: 1.3618 - val_acc: 0.6500\n",
      "Epoch 109/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0487 - acc: 0.9917 - val_loss: 1.2772 - val_acc: 0.6750\n",
      "Epoch 110/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0226 - acc: 0.9983 - val_loss: 1.3332 - val_acc: 0.6850\n",
      "Epoch 111/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0716 - acc: 0.9800 - val_loss: 1.2858 - val_acc: 0.7050\n",
      "Epoch 112/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0283 - acc: 0.9967 - val_loss: 1.4284 - val_acc: 0.6600\n",
      "Epoch 113/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0700 - acc: 0.9817 - val_loss: 1.2995 - val_acc: 0.6900\n",
      "Epoch 114/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0289 - acc: 0.9967 - val_loss: 1.3454 - val_acc: 0.6650\n",
      "Epoch 115/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0390 - acc: 0.9950 - val_loss: 1.2637 - val_acc: 0.6950\n",
      "Epoch 116/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0225 - acc: 0.9983 - val_loss: 1.3110 - val_acc: 0.6800\n",
      "Epoch 117/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0345 - acc: 0.9900 - val_loss: 1.3128 - val_acc: 0.6950\n",
      "Epoch 118/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0246 - acc: 0.9950 - val_loss: 1.3235 - val_acc: 0.6750\n",
      "Epoch 119/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0207 - acc: 0.9983 - val_loss: 1.3738 - val_acc: 0.6600\n",
      "Epoch 120/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0281 - acc: 0.9983 - val_loss: 1.3688 - val_acc: 0.6700\n",
      "Epoch 121/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 46us/step - loss: 0.0216 - acc: 0.9983 - val_loss: 1.3515 - val_acc: 0.6750\n",
      "Epoch 122/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0192 - acc: 0.9967 - val_loss: 1.3489 - val_acc: 0.6950\n",
      "Epoch 123/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0214 - acc: 0.9967 - val_loss: 1.3331 - val_acc: 0.7000\n",
      "Epoch 124/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0192 - acc: 0.9967 - val_loss: 1.3187 - val_acc: 0.7000\n",
      "Epoch 125/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0164 - acc: 0.9983 - val_loss: 1.3275 - val_acc: 0.6850\n",
      "Epoch 126/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0163 - acc: 0.9983 - val_loss: 1.3459 - val_acc: 0.6800\n",
      "Epoch 127/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0172 - acc: 0.9983 - val_loss: 1.3537 - val_acc: 0.6750\n",
      "Epoch 128/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0162 - acc: 0.9983 - val_loss: 1.3660 - val_acc: 0.6700\n",
      "Epoch 129/200\n",
      "600/600 [==============================] - 0s 51us/step - loss: 0.0147 - acc: 0.9983 - val_loss: 1.3824 - val_acc: 0.6600\n",
      "Epoch 130/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0147 - acc: 0.9983 - val_loss: 1.3889 - val_acc: 0.6700\n",
      "Epoch 131/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0140 - acc: 0.9983 - val_loss: 1.3965 - val_acc: 0.6800\n",
      "Epoch 132/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0160 - acc: 0.9983 - val_loss: 1.3919 - val_acc: 0.6850\n",
      "Epoch 133/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0150 - acc: 0.9983 - val_loss: 1.3806 - val_acc: 0.7000\n",
      "Epoch 134/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0128 - acc: 1.0000 - val_loss: 1.3752 - val_acc: 0.6950\n",
      "Epoch 135/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0146 - acc: 0.9983 - val_loss: 1.3835 - val_acc: 0.6950\n",
      "Epoch 136/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0186 - acc: 0.9983 - val_loss: 1.3853 - val_acc: 0.6950\n",
      "Epoch 137/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0163 - acc: 0.9983 - val_loss: 1.3881 - val_acc: 0.6700\n",
      "Epoch 138/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0121 - acc: 0.9983 - val_loss: 1.4489 - val_acc: 0.6600\n",
      "Epoch 139/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0250 - acc: 0.9983 - val_loss: 1.4576 - val_acc: 0.6700\n",
      "Epoch 140/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0207 - acc: 0.9983 - val_loss: 1.4408 - val_acc: 0.6800\n",
      "Epoch 141/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0120 - acc: 0.9983 - val_loss: 1.4905 - val_acc: 0.6800\n",
      "Epoch 142/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0407 - acc: 0.9917 - val_loss: 1.4010 - val_acc: 0.6700\n",
      "Epoch 143/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0137 - acc: 0.9967 - val_loss: 1.4862 - val_acc: 0.6500\n",
      "Epoch 144/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0346 - acc: 0.9933 - val_loss: 1.4480 - val_acc: 0.6550\n",
      "Epoch 145/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0195 - acc: 0.9983 - val_loss: 1.4361 - val_acc: 0.6750\n",
      "Epoch 146/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0158 - acc: 0.9983 - val_loss: 1.4800 - val_acc: 0.6750\n",
      "Epoch 147/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0348 - acc: 0.9917 - val_loss: 1.4359 - val_acc: 0.6850\n",
      "Epoch 148/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0198 - acc: 0.9967 - val_loss: 1.4196 - val_acc: 0.6950\n",
      "Epoch 149/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0139 - acc: 0.9967 - val_loss: 1.4907 - val_acc: 0.6800\n",
      "Epoch 150/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0221 - acc: 0.9967 - val_loss: 1.4383 - val_acc: 0.6800\n",
      "Epoch 151/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0164 - acc: 0.9983 - val_loss: 1.4400 - val_acc: 0.6900\n",
      "Epoch 152/200\n",
      "600/600 [==============================] - 0s 99us/step - loss: 0.0153 - acc: 0.9967 - val_loss: 1.4435 - val_acc: 0.6800\n",
      "Epoch 153/200\n",
      "600/600 [==============================] - 0s 60us/step - loss: 0.0130 - acc: 0.9983 - val_loss: 1.4401 - val_acc: 0.6850\n",
      "Epoch 154/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0114 - acc: 0.9983 - val_loss: 1.5035 - val_acc: 0.7000\n",
      "Epoch 155/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0135 - acc: 0.9967 - val_loss: 1.5539 - val_acc: 0.6850\n",
      "Epoch 156/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0169 - acc: 0.9983 - val_loss: 1.4672 - val_acc: 0.7100\n",
      "Epoch 157/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0097 - acc: 0.9983 - val_loss: 1.4953 - val_acc: 0.6800\n",
      "Epoch 158/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0168 - acc: 0.9967 - val_loss: 1.4612 - val_acc: 0.6900\n",
      "Epoch 159/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0120 - acc: 0.9983 - val_loss: 1.4401 - val_acc: 0.7000\n",
      "Epoch 160/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0112 - acc: 0.9983 - val_loss: 1.5285 - val_acc: 0.6900\n",
      "Epoch 161/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0149 - acc: 0.9983 - val_loss: 1.5639 - val_acc: 0.6800\n",
      "Epoch 162/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 1.5555 - val_acc: 0.6900\n",
      "Epoch 163/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0110 - acc: 0.9983 - val_loss: 1.5768 - val_acc: 0.6850\n",
      "Epoch 164/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0148 - acc: 0.9983 - val_loss: 1.5933 - val_acc: 0.6700\n",
      "Epoch 165/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0164 - acc: 0.9983 - val_loss: 1.5717 - val_acc: 0.6700\n",
      "Epoch 166/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0128 - acc: 0.9983 - val_loss: 1.5339 - val_acc: 0.6850\n",
      "Epoch 167/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0093 - acc: 0.9983 - val_loss: 1.5110 - val_acc: 0.7000\n",
      "Epoch 168/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0097 - acc: 0.9983 - val_loss: 1.5054 - val_acc: 0.6750\n",
      "Epoch 169/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0103 - acc: 0.9983 - val_loss: 1.5058 - val_acc: 0.6900\n",
      "Epoch 170/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0093 - acc: 0.9983 - val_loss: 1.5119 - val_acc: 0.7000\n",
      "Epoch 171/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0090 - acc: 0.9983 - val_loss: 1.5325 - val_acc: 0.6800\n",
      "Epoch 172/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0121 - acc: 0.9983 - val_loss: 1.5445 - val_acc: 0.6700\n",
      "Epoch 173/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0110 - acc: 0.9983 - val_loss: 1.5423 - val_acc: 0.6900\n",
      "Epoch 174/200\n",
      "600/600 [==============================] - 0s 98us/step - loss: 0.0080 - acc: 0.9983 - val_loss: 1.5445 - val_acc: 0.6850\n",
      "Epoch 175/200\n",
      "600/600 [==============================] - 0s 56us/step - loss: 0.0078 - acc: 0.9983 - val_loss: 1.5487 - val_acc: 0.6900\n",
      "Epoch 176/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0092 - acc: 0.9983 - val_loss: 1.5461 - val_acc: 0.6850\n",
      "Epoch 177/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0088 - acc: 0.9983 - val_loss: 1.5393 - val_acc: 0.6850\n",
      "Epoch 178/200\n",
      "600/600 [==============================] - 0s 49us/step - loss: 0.0073 - acc: 0.9983 - val_loss: 1.5312 - val_acc: 0.6900\n",
      "Epoch 179/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0067 - acc: 0.9983 - val_loss: 1.5288 - val_acc: 0.6900\n",
      "Epoch 180/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 1.5311 - val_acc: 0.6800\n",
      "Epoch 181/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600/600 [==============================] - 0s 43us/step - loss: 0.0074 - acc: 0.9983 - val_loss: 1.5402 - val_acc: 0.6750\n",
      "Epoch 182/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0067 - acc: 0.9983 - val_loss: 1.5562 - val_acc: 0.6800\n",
      "Epoch 183/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0066 - acc: 0.9983 - val_loss: 1.5704 - val_acc: 0.6750\n",
      "Epoch 184/200\n",
      "600/600 [==============================] - 0s 48us/step - loss: 0.0078 - acc: 0.9983 - val_loss: 1.5801 - val_acc: 0.6800\n",
      "Epoch 185/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0097 - acc: 0.9983 - val_loss: 1.5801 - val_acc: 0.6750\n",
      "Epoch 186/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0095 - acc: 0.9983 - val_loss: 1.5796 - val_acc: 0.6850\n",
      "Epoch 187/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0081 - acc: 0.9983 - val_loss: 1.5756 - val_acc: 0.6700\n",
      "Epoch 188/200\n",
      "600/600 [==============================] - 0s 44us/step - loss: 0.0064 - acc: 0.9983 - val_loss: 1.5708 - val_acc: 0.6700\n",
      "Epoch 189/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0063 - acc: 0.9983 - val_loss: 1.5661 - val_acc: 0.6700\n",
      "Epoch 190/200\n",
      "600/600 [==============================] - 0s 50us/step - loss: 0.0073 - acc: 0.9983 - val_loss: 1.5603 - val_acc: 0.6800\n",
      "Epoch 191/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0059 - acc: 0.9983 - val_loss: 1.5736 - val_acc: 0.6850\n",
      "Epoch 192/200\n",
      "600/600 [==============================] - 0s 42us/step - loss: 0.0108 - acc: 0.9983 - val_loss: 1.5894 - val_acc: 0.6800\n",
      "Epoch 193/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0150 - acc: 0.9983 - val_loss: 1.5697 - val_acc: 0.6850\n",
      "Epoch 194/200\n",
      "600/600 [==============================] - 0s 45us/step - loss: 0.0089 - acc: 0.9983 - val_loss: 1.5743 - val_acc: 0.6750\n",
      "Epoch 195/200\n",
      "600/600 [==============================] - 0s 52us/step - loss: 0.0091 - acc: 0.9983 - val_loss: 1.5927 - val_acc: 0.6850\n",
      "Epoch 196/200\n",
      "600/600 [==============================] - 0s 43us/step - loss: 0.0165 - acc: 0.9967 - val_loss: 1.5807 - val_acc: 0.6700\n",
      "Epoch 197/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0104 - acc: 0.9983 - val_loss: 1.5909 - val_acc: 0.6750\n",
      "Epoch 198/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0056 - acc: 0.9983 - val_loss: 1.6335 - val_acc: 0.6750\n",
      "Epoch 199/200\n",
      "600/600 [==============================] - 0s 46us/step - loss: 0.0106 - acc: 0.9983 - val_loss: 1.6546 - val_acc: 0.6700\n",
      "Epoch 200/200\n",
      "600/600 [==============================] - 0s 47us/step - loss: 0.0136 - acc: 0.9983 - val_loss: 1.6260 - val_acc: 0.6800\n",
      "200/200 [==============================] - 0s 59us/step\n"
     ]
    }
   ],
   "source": [
    "model = models.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "model.add(layers.Dense(256, activation='relu'))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(partial_x_train,\n",
    "          partial_y_train,\n",
    "          epochs=200,\n",
    "          batch_size=512,\n",
    "          validation_data=(x_val, y_val))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dljqHfDPI6lH"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Mvi9it1SI4aR",
    "outputId": "98b01ef2-3935-442b-82d6-45f56e036d39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.911709065437317, 0.64]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r3hb8s1l4rBA"
   },
   "source": [
    "## Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gudBAhIXJIi2"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Xb7bVPSwJQF0",
    "outputId": "aca09c75-1d21-4847-bdd9-a0521dc8d948"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "llusRQV0JRy9",
    "outputId": "a856289d-883a-47cb-c0fb-ec148330a60a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "0eoEuSZqJTdU",
    "outputId": "94c17d00-dd7f-40a1-84d2-78d1ebde6103"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Utgt1bXfJVRN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "include_colab_link": true,
   "name": "Untitled9.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "notebook",
   "language": "python",
   "name": "notebook"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
